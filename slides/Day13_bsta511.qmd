---
title: "DRAFT - Day 17: Chi-squared tests (Sections 8.3-8.4)"
subtitle: "BSTA 511/611"
author: "Meike Niederhausen, PhD"
institute: "OHSU-PSU School of Public Health"
date: "11/13/2023"
categories: ["Week 8"]
format: 
  revealjs:
      incremental: false
      scrollable: true
      chalkboard: true
      theme: [../sky_modified_smaller_font.scss]
      width:  1100 #1200 # 1050 #default 1050; ipad 3:4, 1600
      height: 825 #900 #800 #default 700; 788 for 3:4, 1200
      slide-number: true
      html-math-method: mathjax
  # html:
  #   link-external-newwindow: true
  #   toc: true
execute:
  echo: true
  freeze: auto  # re-render only when source changes
# editor: visual
editor_options: 
  chunk_output_type: console
---

```{r}
#| label: "setup" 
#| include: false
#| message: false
#| warning: false

library(tidyverse)    
library(oibiostat)
library(janitor)
library(rstatix)
library(knitr)
library(gtsummary)
library(moderndive)
library(gt)
library(broom) 
library(here) 
library(pwr) 

# terminal: for icons
# quarto install extension quarto-ext/fontawesome

# set ggplot theme for slides 
theme_set(theme_bw(base_size = 22))
# theme_update(text = element_text(size=16))  # set global text size for ggplots

```


## MoRitz's tip of the day

__Add text to a plot__ using `annotate()`:


```{r}
#| fig.height: 3.0
ggplot(NULL, aes(c(0,4))) +  # no dataset, create axes for x from 0 to 4
  geom_area(stat = "function", fun = dchisq, args = list(df=2), 
            fill = "blue", xlim = c(0, 1.0414)) +
  geom_area(stat = "function", fun = dchisq, args = list(df=2),
            fill = "violet", xlim = c(1.0414, 4)) +
  geom_vline(xintercept = 1.0414) +  # vertical line at x = 1.0414
  annotate("text", x = 1.1, y = .4, # add text at specified (x,y) coordinate
           label = "chi-squared = 1.0414", hjust=0, size=6) + 
  annotate("text", x = 1.3, y = .1, 
           label = "p-value = 0.59", hjust=0, size=6) 
```




## Where are we?

* We've covered inference for
    * continuous outcomes: 
        * mean for one group
        * mean difference of two paired groups
        * difference in means of two independent groups
    * binary outcomes:
        * proportion for one group
        * difference in proportions of two independent groups
        * BSTA 513: 
            * inference for paired proportions (Mc Nemar's test)

<hr>

* What if we want to 
    * compare more than two groups?
    * or have categorical outcome that has more than 2 levels?






## Goals for today (Sections 8.3-8.4)


* Statistical inference for __categorical data__ when either are 
    * comparing __more than two groups__ or
    * have categorical outcomes that have __more than 2 levels__ or
    * both

* Chi-squared tests of association (independence)
    * Hypotheses
    * test statistic
    * Chi-squared distribution
    * p-value
    * technical assumptions (conditions)
    * conclusion
    * R: `chisq.test()`

* Fisher's Exact Test
* Chi-squared test vs. testing difference in proportions
    * Test of Homogeneity



# Chi-squared tests of association (independence)

Testing the association (independence) between two categorical variables



## Is there an association between depression and being physically active?

* I sampled data from the NHANES R package:
    * American National Health and Nutrition Examination Surveys
    * Data collected 2009-2012 by US National Center for Health Statistics (NCHS) 
    * `NHANES` dataset: 10,000 rows, resampled from NHANESraw to undo oversampling effects 
        * Treat it as a simple random sample from the US population (for educational purposes)


* __Depressed__
    * Self-reported number of days where participant felt down, depressed or hopeless. 
    * One of None, Several, Majority (more than half the days), or AlmostAll.
    * Reported for participants aged 18 years or older. 

* __PhysActive__
    * Participant does moderate or vigorous-intensity sports, fitness or recreational activities (Yes or No). 
    * Reported for participants 12 years or older.







## Hypotheses for a Chi-squared test of association (independence)

::: columns
::: {.column width="50%"}
__Generic wording:__

Test of "__association__" wording

- $H_0$:  There is no association between the two variables  

- $H_A$:  There is an association between the two variables 

Test of "__independence__" wording

- $H_0$:  The variables are independent

- $H_A$:  The variables are not independent

:::

::: {.column width="50%"}

__For our example:__

Test of "__association__" wording

- $H_0$:  There is no association between depression and physical activity

- $H_A$: There is an association between depression and physical activity

Test of "__independence__" wording

- $H_0$:  The variables depression and physical activity are independent

- $H_A$: The variables depression and physical activity are not independent

:::
:::



## Data from NHANES

* Results below are from 
    * a random sample of 400 adults (â‰¥ 18 yrs old) 
    * with data for both the depression `Depressed` and physically active (`PhysActive`) variables.

![](/img_slides/ChiSq_Table_Obs_DepressionPA.png){fig-align="center"}

* What does it mean for the variables to be independent?




## $H_0$: Variables are Independent

::: columns
::: {.column width="60%"}
* Recall from Chapter 2, that events $A$ and $B$ are independent if and only if

$$P(A~and~B)=P(A)P(B)$$

* If depression and being physically active are independent variables, then _theoretically_ this condition needs to hold for every combination of levels, i.e.

$$P(None~and~Yes)=P(None)P(Yes)\\
P(None~and~No)=P(None)P(No)\\
P(Several~and~Yes)=P(Several)P(Yes)\\
P(Several~and~No)=P(Several)P(No)\\
P(Most~and~Yes)=P(Most)P(Yes)\\
P(Most~and~No)=P(Most)P(No)$$
:::

::: {.column width="40%"}
![](/img_slides/ChiSq_Table_Obs_DepressionPA.png){fig-align="center"}

$$P(None~and~Yes)=\frac{314}{400}\cdot\frac{226}{400}\\
...\\
P(Most~and~No)=\frac{28}{400}\cdot\frac{174}{400}$$

With these probabilities, for each cell of the table
we calculate the __expected__ counts for each cell under the $H_0$ hypothesis that the variables are independent

:::
:::


## Expected counts (if variables are independent)

::: columns
::: {.column width="60%"}
+ The expected counts (if $H_0$ is true & the variables are independent) for each cell are
    + $np$ = total table size $\cdot$ probability of cell

Expected count of Yes & None:

$$400 \cdot P(None~and~Yes)\\
=400 \cdot P(None)P(Yes)\\
=400 \cdot\frac{314}{400}\cdot\frac{226}{400}\\
=\frac{314\cdot 226}{400} = 177.41\\
=\frac{column~total\cdot row~total}{table~total}$$

:::

::: {.column width="40%"}
![](/img_slides/ChiSq_Table_Obs_DepressionPA.png){fig-align="center"}


* If depression and being physically active are independent variables 
  * (as assumed by $H_0$), 
* then the __observed counts should be close to the expected counts__ for each cell of the table


:::
:::


## Observed vs. Expected counts

::: columns
::: {.column width="40%"}
* The __observed__ counts are the counts in the 2-way table summarizing the data

![](/img_slides/ChiSq_Table_Obs_DepressionPA.png){fig-align="center"}

Expected count for cell $i,j$ :
:::

::: {.column width="60%"}
* The __expected__ counts are the counts the we would expect to see in the 2-way table if there was no association between depression and being physically activity

![](/img_slides/ChiSq_Table_Expected_DepressionPA.png){fig-align="center"}

:::
:::

$$\textrm{Expected Count}_{\textrm{row } i,\textrm{ col }j}=\frac{(\textrm{row}~i~ \textrm{total})\cdot(\textrm{column}~j~ \textrm{total})}{\textrm{table total}}$$


## The $\chi^2$ test statistic

::: columns
::: {.column width="40%"}
Test statistic for a test of association (independence):

$$\chi^2 = \sum_{\textrm{all cells}} \frac{(\textrm{observed} - \text{expected})^2}{\text{expected}}$$
* When the variables are independent, the observed and expected counts should be close to each other

:::

::: {.column width="60%"}
![](/img_slides/ChiSq_Table_Expected_brief_DepressionPA.png){fig-align="center"}
:::
:::

<hr>

$$\chi^2 =  \sum\frac{(O-E)^2}{E} = \frac{(199-177.41)^2}{177.41} + \frac{(26-32.77)^2}{32.77} + \ldots + \frac{(27-12.18)^2}{12.18} =  41.2$$

Is this big? Big enough to reject $H_0$?



## The $\chi^2$ distribution & calculating the _p_-value

::: columns
::: {.column width="60%"}
The $\chi^2$ distribution shape depends on its degrees of freedom

* It's skewed right for smaller df,
    * gets more symmetirc for larger df
* __<span style="color:green">df = (# rows-1) x (# columns-1)</span>__

![](/img_slides/chisq_density.png){fig-align="center"}
:::

::: {.column width="40%"}
* The __p-value__ is always the __area to the right__ of the test statistic for a $\chi^2$ test.
* We can use the `pchisq` function in R to calculate the probability of being at least as big as the $\chi^2$ test statistic:

```{r}
pv <- pchisq(41.2, df = 2, 
       lower.tail = FALSE)
pv
```

What's the conclusion to the $\chi^2$ test?
:::
:::


## Conclusion

::: columns
::: {.column width="50%"}
Recall the hypotheses to our $\chi^2$ test:

- $H_0$:  There is __no association__ between depression and being physically activity

- $H_A$: There is an association between depression and being physically activity

```{r}
#| fig.height: 3.5
#| echo: false
ggplot(NULL, aes(c(0,50))) + # no dataset, create axes for x from 0 to 50
  # Draw & fill in chi-squared distribution from 0 to 41.2 (test stat value)
  geom_area(stat = "function", fun = dchisq, args = list(df=2),fill = "skyblue", alpha =0.9, xlim = c(0, 41.2)) +
  # Draw & fill in chi-squared distribution from 41.2 to 50
  geom_area(stat = "function", fun = dchisq, args = list(df=2),fill = "violet", alpha =0.4, xlim = c(41.2, 50)) +
  labs(x = "", y = "", title="Chi-squared test p-value") +
  scale_y_continuous(breaks = NULL) +
  scale_x_continuous(breaks = seq(0, 50, by=10)) + # x-axis tick marks: 0,10,20,...,50
  geom_vline(xintercept = 41.2) + # vertical line at x = 41.2 (test stat)
  annotate("text", x = 30, y = .4, # add text at specified (x,y) coordinate
           # label is text that will appear
           label = "chi-squared = 41.2", hjust=0, size=6) +
  annotate("text", x = 30, y = .1, label = "p-value < 0.001", hjust=0, size=6) +
  theme(panel.grid.major = element_blank(), # remove the major grid lines
        panel.grid.minor = element_blank()) # remove the minor grid lines
```


:::

::: {.column width="50%"}
__Conclusion:__

There is sufficient evidence that there is an association between depression and being physically activity (_p_-value < 0.001).
:::
:::



## Technical assumptions


* __Independence__
    * Each case (person) that contributes a count to the table must be independent of all the other cases in the table
        * In particular, cases cannot be in two categories
        * For example, someone cannot choose both "Several" and "Most" for depression status. They have to choose exactly one option for each variable.

<hr>
::: columns
::: {.column width="60%"}
* __Sample size__
    * In order for the test statistic to be modeled by a chi-squared distribution we need
    * __<span style="color:darkorange"> 2 $\times$ 2 table:</span> expected counts are at least 10 for each cell__
    * __<span style="color:darkorange">larger tables:   </span>__
        * __no more than 1/5 of the expected counts are less than 5__, and 
        * __all expected counts are greater than 1__
:::

::: {.column width="40%"}
![](/img_slides/ChiSq_Table_Expected_brief_DepressionPA.png){fig-align="center"}
:::
:::



# Chi-squared tests in R


## Depression vs. physical activity dataset

::: columns
::: {.column width="50%"}

 Create dataset based on results table:
```{r}
DepPA <- tibble(
  Depression = c(rep("None", 314), 
         rep("Several", 58),
         rep("Most", 28)),
  PA = c(rep("Yes", 199),  # None
          rep("No", 115),
          rep("Yes", 26), # Several
          rep("No", 32),
          rep("Yes", 1), # Most
          rep("No", 27))
)
```

:::

::: {.column width="50%"}
 ![](/img_slides/ChiSq_Table_Expected_brief_DepressionPA.png){fig-align="center"}
 
Summary table of data:
```{r}
DepPA %>% tabyl(Depression, PA) 
```
:::
:::


## $\chi^2$ test in R using dataset


```{r}
(ChisqTest_DepPA <- chisq.test(DepPA$PA, DepPA$Depression))
```


```{r}
# `tidy()` the output (from `broom` package):
tidy(ChisqTest_DepPA)
tidy(ChisqTest_DepPA)$p.value
```



## Observed & expected counts in R

::: columns
::: {.column width="60%"}

You can see what the __observed__ and __expected__ counts are from the saved chi-squared test results:
```{r}
ChisqTest_DepPA$observed
ChisqTest_DepPA$expected
```
:::

::: {.column width="40%"}

![](/img_slides/ChiSq_Table_Expected_brief_DepressionPA.png){fig-align="center"}

* Why is it important to look at the expected counts?

* What are we looking for in the expected counts?

:::
:::



## $\chi^2$ test in R with 2-way table (1/2)

Create a base R table of the results:

```{r}
(DepPA_table <- matrix(c(199, 26, 1, 115, 32, 27), nrow = 2, ncol = 3, byrow = T))
dimnames(DepPA_table) <- list("PA" = c("Yes", "No"),   # row names
                              "Depression" = c("None", "Several", "Most"))  # column names
DepPA_table
```





## $\chi^2$ test in R with 2-way table (2/2)

```{r}
chisq.test(DepPA_table) 
chisq.test(DepPA_table)$expected
```



## Continuity correction

* Just like the proportions test, $\chi^2$ tests have the option of including a continuity correction
* The __default includes a continuity correction__
    * Output below is without a CC - it looks the same for this example 

```{r}
chisq.test(DepPA_table, correct = FALSE) %>% tidy()
```




# Fischer's Exact Test

Use this if sample size too small



## Example with small sample sizes 

* Suppose that instead of taking a random sample of 400 adults (from the NHANES data), a study takes a random sample of 100 such that
    * 50 people that are physically active and 
    * 50 people that are not physically active 

```{r}
(DepPA100_table <- matrix(c(43, 5, 2, 40, 4, 6), nrow = 2, ncol = 3, byrow = T))
dimnames(DepPA100_table) <- list("PA" = c("Yes", "No"),   # row names
                              "Depression" = c("None", "Several", "Most"))  # column names
DepPA100_table

```


## Chi-squared test warning (1/2)

```{r}
#| warning: true
chisq.test(DepPA100_table) 
```



## Chi-squared test warning (2/2)

```{r}
#| warning: true
chisq.test(DepPA100_table)$expected
```

* Recall the __sample size__ condition
    * In order for the test statistic to be modeled by a chi-squared distribution we need
    * __<span style="color:darkorange"> 2 $\times$ 2 table:</span> expected counts are at least 10 for each cell__
    * __<span style="color:darkorange">larger tables:   </span>__
        * __no more than 1/5 of the expected counts are less than 5__, and 
        * __all expected counts are greater than 1__



## Fisher's Exact Test

* Called an exact test since it 
    * calculates an exact p-value probability 
        * instead of using an asymptotic approximation, such as the normal, t, or chi-squared distributions
    * For 2x2 tables the p-value is calculated using the hypergeometric probability distribution (see book for details)  


```{r}
fisher.test(DepPA100_table)
```

Note: no test statistic & a two-sided test


## Another option: simulate p-value

From the help file: 

* Simulation is done by random sampling from the set of all contingency tables with given marginals, and 
    * works only if the marginals are strictly positive. 
* Continuity correction is never used, and the statistic is quoted without it. 
* Note that this is not the usual sampling situation assumed for the chi-squared test but rather that for Fisher's exact test.

```{r}
chisq.test(DepPA100_table, simulate.p.value = TRUE) 
```





# $\chi^2$ test vs. testing proportions




## $\chi^2$ test vs. testing differences in proportions (1/2)

If there are only 2 levels in both of the categorical variables being tested, then the _p_-value from the $\chi^2$ test is equal to the _p_-value from the differences in proportions test.

__Example:__ On Day 15 we tested whether the proportion who had participated in sports betting was the same for college and noncollege young adults:

$$~~~~H_0: p_{coll} - p_{noncoll} = 0\\
H_A: p_{coll} - p_{noncoll} \neq 0$$


```{r}
SportsBet_table <- matrix(c(175, 94, 137, 77), nrow = 2, ncol = 2, byrow = T)
dimnames(SportsBet_table) <- list("Group" = c("College", "NonCollege"),   # row names
                              "Bet" = c("No", "Yes"))  # column names
SportsBet_table
```




## $\chi^2$ test vs. testing differences in proportions (2/2)

```{r}
chisq.test(SportsBet_table) %>% tidy()
prop.test(SportsBet_table) %>% tidy() 
z <- sqrt(0.0199)
2*pnorm(z, lower.tail=F)# p-value
```

<!-- * Same test statistic (both shown as $\chi^2$) and _p_-value! -->
<!-- * This is because $Z^2 \sim \chi_{df=1}^2$, if $Z\sim N(0,1)$. -->



## Test of Homogeneity

* Running the sports betting example as a chi-squared test is actually an example of a __test of homogeneity__

* In a test of homogeneity, proportions can be compared between many groups

$$~~~~H_0: p_1 = p_2 = p_2 = \ldots = p_n\\
H_A: p_i \neq p_j \textrm{for at least one pair of } i, j$$

* It's an extension of a two proportions test.

* The test statistic & p-value are calculated the same was as a chi-squared test of association (independence)

* When we fix the margins (whether row or columns) of one of the "variables"
    * the chi-squared test is called a __Test of Homogeneity__



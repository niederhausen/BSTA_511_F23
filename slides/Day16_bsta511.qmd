---
title: "DRAFT Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)"
subtitle: "BSTA 511/611"
author: "Meike Niederhausen, PhD"
institute: "OHSU-PSU School of Public Health"
date: "11/27/2023"
categories: ["Week 10"]
format: 
  revealjs:
      incremental: false
      scrollable: true
      chalkboard: true
      theme: [../sky_modified_smaller_font.scss]
      width:  1100 #1200 # 1050 #default 1050; ipad 3:4, 1600
      height: 825 #900 #800 #default 700; 788 for 3:4, 1200
      slide-number: true
      html-math-method: mathjax
  # html:
  #   link-external-newwindow: true
  #   toc: true
execute:
  echo: true
  freeze: auto  # re-render only when source changes
# editor: visual
editor_options: 
  chunk_output_type: console
---

```{r}
#| label: "setup" 
#| include: false
#| message: false
#| warning: false

library(tidyverse)    
library(oibiostat)
library(janitor)
library(rstatix)
library(knitr)
library(gtsummary)
library(moderndive)
library(gt)
library(broom) 
library(here) 
library(pwr) 
library(gridExtra) # NEW!!!

# terminal: for icons
# quarto install extension quarto-ext/fontawesome

# set ggplot theme for slides 
theme_set(theme_bw(base_size = 22))
# theme_update(text = element_text(size=16))  # set global text size for ggplots

```



## Where are we? 

<br>
<br>

![](/img_slides/flowchart_only_continuous.jpg){fig-align="center"}


## Goals for today (Sections 6.3-6.4)

* Simple Linear Regression Part 2 

+ Best-fit line (aka regression line or least-squares line)
+ Coefficient interpretation
+ Prediction
+ Residuals
+ LINE assumptions

## Life expectancy vs. female adult literacy rate

<https://www.gapminder.org/tools/#$model$markers$bubble$encoding$x$data$concept=literacy_rate_adult_female_percent_of_females_ages_15_above&source=sg&space@=country&=time;;&scale$domain:null&zoomed:null&type:null;;&frame$value=2011;;;;;&chart-type=bubbles&url=v1>

![](/img_slides/gapminder_ex.png){fig-align="center"}


## Dataset description 

* Data file: `lifeexp_femlit_water_2011.csv`
* Data were downloaded from <https://www.gapminder.org/data/>
* 2011 is the most recent year with the most complete data
* __Life expectancy__ = the average number of years a newborn child would live if current mortality patterns were to stay the same. Source: <https://www.gapminder.org/data/documentation/gd004/>

* __Adult literacy rate__ is the percentage of people ages 15 and above who can, with understanding, read and write a short, simple statement on their everyday life. Source: <http://data.uis.unesco.org/>

* __At least basic water source (%)__ = the percentage of people using at least basic water services. This indicator encompasses both people using basic water services as well as those using safely managed water services. Basic drinking water services is defined as drinking water from an improved source, provided collection time is not more than 30 minutes for a round trip. Improved water sources include piped water, boreholes or tubewells, protect dug wells, protected springs, and packaged or delivered water.


## Get to know the data

Load data

```{r}
gapm_original <- read_csv(here::here("data", "lifeexp_femlit_water_2011.csv"))
```

Glimpse of the data

```{r}
glimpse(gapm_original)
```

Note the missing values for our variables of interest

```{r}
gapm_original %>% select(life_expectancy_years_2011, female_literacy_rate_2011) %>% 
  get_summary_stats()
```



## Remove missing values


Remove rows with missing data for life expectancy and female literacy rate

```{r}
gapm <- gapm_original %>% 
  drop_na(life_expectancy_years_2011, female_literacy_rate_2011)

glimpse(gapm)

```

No missing values now for our variables of interest

```{r}
gapm %>% select(life_expectancy_years_2011, female_literacy_rate_2011) %>% 
  get_summary_stats()
```


:::{.callout-important}
* Removing the rows with missing data was not needed to run the regression model.
* I did this step since later we will be calculating the standard deviations of the explanatory and response variables for _just the values included in the regression model_. It'll be easier to do this if we remove the missing values now. 
:::


## Association between life expectancy & female literacy rate

::: columns
::: {.column width="55%"}


```{r}
#| label: assoc
#| fig.show: hide
#| fig.height: 6.0
#| fig.width: 6.0
ggplot(gapm, 
       aes(x = female_literacy_rate_2011,
           y = life_expectancy_years_2011)) +
  geom_point() +
  labs(x = "female literacy rate", 
       y = "life expectancy",
       title = "Life expectancy vs. 
             female literacy rate") +  
  geom_smooth(method = "lm", 
              se = FALSE)
```

* Is there a relationship between the two variables? 
* Is it positive or negative? 
* Strong, moderate, or weak? 
* Is it linear?

:::

::: {.column width="45%"}
```{r}
#| ref.label: assoc
#| echo: false
#| fig.height: 8.0
#| fig.width: 8.0

```
:::
:::



## Regression line = best-fit line

::: columns
::: {.column width="60%"}

$$\widehat{y} = b_0 + b_1 \cdot x $$

* $\hat{y}$ is the predicted outcome for a specific value of $x$.
* $b_0$ is the intercept
* $b_1$ is the slope of the line, i.e., the increase in $\hat{y}$ for every increase of one (unit increase) in $x$.
    - slope = *rise over run*


:::

::: {.column width="40%"}

```{r}
#| echo: false
#| fig.height: 6.0
#| fig.width: 8.0
ggplot(gapm, aes(x = female_literacy_rate_2011,
                 y = life_expectancy_years_2011)) +
  geom_point() +
  labs(x = "female literacy rate", 
       y = "life expectancy",
       title = "Life expectancy vs. female literacy rate") +  
  geom_smooth(method = "lm", se = FALSE)
```
:::
:::

* __Intercept__
    - The expected outcome for the $y$-variable when the $x$-variable is 0.
* __Slope__
    - For every increase of 1 unit in the $x$-variable, there is an expected increase of, on average, $b_1$ units in the $y$-variable.
    - We only say that there is an expected increase and not necessarily a causal increase.



## Residuals

::: columns
::: {.column width="40%"}

* __Observed values__ $y_i$ 
  * the values in the dataset

* __Fitted values__ $\widehat{y}_i$ 
  * the values that fall on the best-fit line for a specific $x_i$

* __Residuals__ $e_i = y_i - \widehat{y}_i$ 
  * the differences between the observed and fitted values

:::

::: {.column width="60%"}

```{r}
#| echo: false
#| fig.height: 7.0
#| fig.width: 8.0
# code from https://drsimonj.svbtle.com/visualising-residuals

model1 <- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,
                 data = gapm)
regression_points <- augment(model1)

ggplot(regression_points, 
       aes(x = female_literacy_rate_2011,
                 y = life_expectancy_years_2011)) +
  geom_smooth(method = "lm", se = FALSE, color = "lightgrey") +
  geom_segment(aes(
    xend = female_literacy_rate_2011, 
    yend = .fitted), 
    alpha = .2) +
  # > Color adjustments made here...
  geom_point(aes(color = .resid), size = 4) +  # Color mapped here
  scale_color_gradient2(low = "blue", mid = "white", high = "red") +  # Colors to use here
  guides(color = "none") +
  geom_point(aes(y = .fitted), shape = 1, size = 3) +
  labs(x = "Female literacy rate", 
       y = "Average life expectancy",
       title = "Regression line with residuals") +
  theme_bw() + 
  theme(text = element_text(size = 20))    
```

:::
:::

## Regression in R: `lm()`, `summary()`, & `tidy()`

```{r}
model1 <- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,
                 data = gapm)

summary(model1)

tidy(model1) %>% gt()
```


---

## Regression equation

The values in the `estimate` column are the intercept and slope of the regression model.

```{r}
#| echo: false
tidy(model1) %>% gt()
```


Generic regression equation:

$$\widehat{y} = b_0 + b_1 \cdot x $$

Regression equation for our model:

$$\widehat{\textrm{life expectancy}} = 50.9 + 0.232 \cdot \textrm{female literacy rate} $$

## The (population) regresison model

* The (population) regression model is denoted by

$$y = \beta_0 + \beta_1 \cdot x + \epsilon$$

* $\beta_0$ and $\beta_1$ are unknown population parameters
* $\epsilon$ (epsilon) is the error about the line
    * It is assumed to be a random variable with a 
        * normal distribution with
        * mean 0 and
        * constant variance $\sigma^2$

* The __line__ is the average (expected) value of $Y$ given a value of $x$: $E(Y|x)$.

* The point estimates based on a sample are denoted by $b_0, b_1, s_{residuals}^2$
    * Note: also common notation is $\widehat{\beta}_0, \widehat{\beta}_1, \widehat{\sigma}^2$
    
    
    
## The (population) regresison model visually

$$y = \beta_0 + \beta_1 \cdot x + \epsilon, \text{ where } \epsilon \sim N(0, \sigma^2)$$

![](/img_slides/OLSassumptions-1.png){fig-align="center"}

::: {style="font-size: 60%;"}
<https://bookdown.org/roback/bookdown-bysh/ch-MLRreview.html#ordinary-least-squares-ols-assumptions>
:::





## What are the LINE conditions?

For "good" model fit and to be able to make inferences and predictions based on our models, 4 conditions need to be satisfied.  

Briefly: 

* __L__ inearity of relationship between variables
* __I__ ndependence of the Y values
* __N__ ormality of the residuals
* __E__ quality of variance of the residuals (homoscedasticity)

[More in depth](https://bookdown.org/roback/bookdown-bysh/ch-MLRreview.html#ordinary-least-squares-ols-assumptions):

* __L__ : there is a linear relationship between the mean response (Y) and the explanatory variable (X),
* __I__ : the errors are independent—there’s no connection between how far any two points lie from the regression line,
* __N__ : the responses are normally distributed at each level of X, and
* __E__ : the variance or, equivalently, the standard deviation of the responses is equal for all levels of X.


## L: Linearity of relationship between variables

Is the association between the variables linear?

* Diagnostic tools:
    * Scatterplot
    * Residual plot (see later section for E : Equality of variance of the residuals)

```{r}
#| echo: false
#| fig.width: 12.0
#| fig.height: 7.0
ggplot(gapm, aes(x = female_literacy_rate_2011,
                 y = life_expectancy_years_2011)) +
  geom_point() +
  labs(x = "female literacy rate", 
       y = "life expectancy",
       title = "Life expectancy vs. female literacy rate in 2011") +  
  geom_smooth(method = "lm", se = FALSE) +
  geom_smooth(se = FALSE, color = "black")
```




## I: Independence of the residuals ($Y$ values)

* __Are the data points independent of each other?__

* Examples of when they are *not* independent, include
    * repeated measures (such as baseline, 3 months, 6 months)
    * data from clusters, such as different hospitals or families

* This condition is checked by reviewing the study *design* and not by inspecting the data

* How to analyze data using regression models when the $Y$-values are not independent is covered in BSTA 519 (Longitudinal data) 


# N: Normality of the residuals

* Extract residuals from regression model in R
* Distribution plots of residuals
* QQ plots

## N: Normality of the residuals

* The responses Y are normally distributed at each level of x

![](/img_slides/OLSassumptions-1.png){fig-align="center"}

::: {style="font-size: 60%;"}
<https://bookdown.org/roback/bookdown-bysh/ch-MLRreview.html#ordinary-least-squares-ols-assumptions>
:::


## Extract model's residuals in R

* First extract the residuals' values from the model output using the `augment()` function from the `broom` package.
* Get a tibble with the orginal data, as well as the residuals and some other important values.

```{r}
model1 <- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011, 
                data = gapm)
aug1 <- augment(model1) 

glimpse(aug1)
```


## Check normality with "usual" distribution plots

Note that below I save each figure, and then combine them together in one row of output using `grid.arrange()` from the `gridExtra` package.

```{r}
#| fig.height: 4.0
#| fig.width: 12.0
hist1 <- ggplot(aug1, aes(x = .resid)) +
  geom_histogram()

density1 <- ggplot(aug1, aes(x = .resid)) +
  geom_density()

box1 <- ggplot(aug1, aes(x = .resid)) +
  geom_boxplot()

library(gridExtra) # NEW!!!
grid.arrange(hist1, density1, box1, nrow = 1)
```


## Normal QQ plots (QQ = quantile-quantile)

* It can be tricky to eyeball with a histogram or density plot whether the residuals are normal or not
* QQ plots are often used to help with this

::: columns
::: {.column width="60%"}

* _Vertical axis_: __data quantiles__
  * data points are sorted in order and 
  * assigned quantiles based on how many data points there are
* _Horizontal axis_: __theoretical quantiles__
    * mean and standard deviation (SD) calculated from the data points
    * theoretical quantiles are calculated for each point, assuming the data are modeled by a normal distribution with the mean and SD of the data

:::

::: {.column width="40%"}
```{r}
#| fig.width: 5.0
#| fig.height: 5.0
#| echo: false
ggplot(aug1, aes(sample = .resid)) + 
  stat_qq() +     # points
  stat_qq_line()  # line
```
:::
:::

* __Data are approximately normal if points fall on a line.__

See more info at <https://data.library.virginia.edu/understanding-QQ-plots/> 



## Examples of Normal QQ plots (1/5)

* __Data__: 
    * Body measurements from 507 physically active individuals 
    * in their 20's or early 30's 
    * within normal weight range. 

![](/img_slides/qq_wristdiam.png){fig-align="center"}


## Examples of Normal QQ plots (2/5)

Skewed right distribution

<br>

![](/img_slides/qq_weights.png){fig-align="center"}



## Examples of Normal QQ plots (3/5)

Long tails in distribution

<br>

![](/img_slides/qq_biliac.png){fig-align="center"}



## Examples of Normal QQ plots (4/5)

Bimodal distribution

<br>

![](/img_slides/qq_forearm.png){fig-align="center"}



## Examples of Normal QQ plots (5/5)

![](/img_slides/qq_forearm_gender.png){fig-align="center"}


## QQ plot of residuals of `model1`

```{r}
#| fig.width: 12.0
#| fig.height: 3.0
#| echo: false
grid.arrange(hist1, density1, box1, nrow = 1)
```

::: columns
::: {.column width="50%"}

<br>

```{r}
#| label: QQresid
#| fig.show: hide
#| fig.width: 5.0
#| fig.height: 5.0

ggplot(aug1, aes(sample = .resid)) + 
  stat_qq() +     # points
  stat_qq_line()  # line
```

:::

::: {.column width="50%"}
```{r}
#| ref.label: QQresid
#| echo: false
#| fig.width: 5.0
#| fig.height: 4.0
```
:::
:::


## Compare to randomly generated Normal QQ plots 


How "_good_" we can expect a QQ plot to look depends on the sample size. 

* The QQ plots on the next slides are randomly generated
    * using random samples from actual normal distributions
* Thus, all the points in the QQ plots __should theoretically__ fall in a line
* However, there is sampling variability...


## Randomly generated Normal QQ plots: n=100 

* Note that `stat_qq_line()` doesn't work with randomly generated samples, and thus the code below manually creates the line that the points should be on (which is $y=x$ in this case.)

::: columns
::: {.column width="50%"}
::: {style="font-size: 90%;"}
```{r}
#| fig.height: 5.0
#| fig.width: 5.0
samplesize <- 100

rand_qq1 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  # line y=x
  geom_abline(intercept = 0, slope = 1, 
              color = "blue") 

rand_qq2 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  geom_abline(intercept = 0, slope = 1, 
              color = "blue")

rand_qq3 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  geom_abline(intercept = 0, slope = 1, 
              color = "blue")

rand_qq4 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  geom_abline(intercept = 0, slope = 1, 
              color = "blue")
```
:::
:::

::: {.column width="50%"}


```{r}
#| fig.height: 8.0
#| fig.width: 8.0

grid.arrange(rand_qq1, rand_qq2, 
             rand_qq3, rand_qq4, ncol =2)
```
:::
:::



## Examples of simulated Normal QQ plots: n=10 

With fewer data points,

* simulated QQ plots are more likely to look "less normal" 
* even though the data points were sampled from normal distributions.

::: columns
::: {.column width="50%"}
::: {style="font-size: 90%;"}
```{r}
#| fig.height: 5.0
#| fig.width: 5.0
samplesize <- 10  # only change made to code!

rand_qq1 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  # line y=x
  geom_abline(intercept = 0, slope = 1, 
              color = "blue") 

rand_qq2 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  geom_abline(intercept = 0, slope = 1, 
              color = "blue")

rand_qq3 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  geom_abline(intercept = 0, slope = 1, 
              color = "blue")

rand_qq4 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  geom_abline(intercept = 0, slope = 1, 
              color = "blue")
```
:::
:::

::: {.column width="50%"}


```{r}
#| fig.height: 8.0
#| fig.width: 8.0

grid.arrange(rand_qq1, rand_qq2, 
             rand_qq3, rand_qq4, ncol =2)
```
:::
:::



## Examples of simulated Normal QQ plots: n=1,000 

With more data points,

* simulated QQ plots are more likely to look "more normal" 

::: columns
::: {.column width="50%"}
::: {style="font-size: 90%;"}
```{r}
#| fig.height: 5.0
#| fig.width: 5.0
samplesize <- 1000 # only change made to code!

rand_qq1 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  # line y=x
  geom_abline(intercept = 0, slope = 1, 
              color = "blue") 

rand_qq2 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  geom_abline(intercept = 0, slope = 1, 
              color = "blue")

rand_qq3 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  geom_abline(intercept = 0, slope = 1, 
              color = "blue")

rand_qq4 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  geom_abline(intercept = 0, slope = 1, 
              color = "blue")
```
:::
:::

::: {.column width="50%"}


```{r}
#| fig.height: 8.0
#| fig.width: 8.0

grid.arrange(rand_qq1, rand_qq2, 
             rand_qq3, rand_qq4, ncol =2)
```
:::
:::



## Back to our example

::: columns
::: {.column width="40%"}
Residuals from Life Expectancy vs. Female Literacy Rate Regression


```{r}
#| label: QQplot
#| fig.show: hide
ggplot(aug1, 
      aes(sample = .resid)) + 
  stat_qq() + 
  stat_qq_line() 
```

:::

::: {.column width="60%"}
```{r}
#| ref.label: QQplot
#| echo: false
```

:::
:::

Simulated QQ plot of Normal Residuals with n = 80  

::: columns
::: {.column width="40%"}

```{r}
# number of observations 
# in fitted model
nobs(model1) 
```

```{r}
#| label: QQplot_sim
#| fig.show: hide
ggplot() +
  stat_qq(aes(
    sample = rnorm(80))) + 
  geom_abline(
    intercept = 0, slope = 1, 
    color = "blue")
```

:::

::: {.column width="60%"}
```{r}
#| ref.label: QQplot_sim
#| echo: false
```

:::
:::




# E: Equality of variance of the residuals {.nostretch}

* Homoscedasticity
* Check using a __residual plot__

![](/img_slides/OLSassumptions-1.png){fig-align="center" width=60%}

::: {style="font-size: 60%;"}
<https://bookdown.org/roback/bookdown-bysh/ch-MLRreview.html#ordinary-least-squares-ols-assumptions>
:::


## Residual plot

* $x$ = explanatory variable from regression model,  
    * or the fitted values for a multiple regression
* $y$ = residuals from regression model


::: columns
::: {.column width="50%"}
```{r}
names(aug1)
```

<br>

```{r}
#| label: resids_plot
#| fig.show: hide
ggplot(aug1, 
       aes(x = female_literacy_rate_2011, 
           y = .resid)) + 
  geom_point() +
  geom_abline(
    intercept = 0, 
    slope = 0, 
    color = "orange") +
  labs(title = "Residual plot")
```

:::

::: {.column width="50%"}
```{r}
#| ref.label: resids_plot
#| echo: false
#| fig.height: 6
#| fig.width: 6
```

:::
:::




## E: Equality of variance of the residuals (Homoscedasticity)

* The __variance__ or, equivalently, the standard deviation of the responses is __equal for all values of x__.
* This is called __homoskedasticity__ (top row)
* If there is __heteroskedasticity__ (bottom row), then the assumption is not met.

![](/img_slides/heteroskedastic.png){fig-align="center"}





# $R^2$ = Coefficient of determination

Another way to assess model fit



## $R^2$ = Coefficient of determination (1/2)

+ Recall that the correlation coefficient $r$ measures the strength of the linear relationship between two numerical variables
+ $R^2$ is usually used to measure the strength of a _linear fit_
  + For a simple linear regression model (one numerical predictor), $R^2$ is just the square of the correlation coefficient
+ In general, $R^2$ is the proportion of the variability of the dependent variable that is __explained__ by the independent variable(s)

$$R^2 = \frac{\textrm{variance of predicted y-values}}
{\textrm{variance of observed y-values}} = \frac{\sum_{i=1}^n(\widehat{y}_i-\bar{y})^2}
{\sum_{i=1}^n(y_i-\bar{y})^2} 
 = \frac{s_y^2 - s_{\textrm{residuals}}^2}
{s_y^2}$$
$$R^2 = 1- \frac{s_{\textrm{residuals}}^2}
{s_y^2}$$
where $\frac{s_{\textrm{residuals}}^2}{s_y^2}$ is the proportion of "unexplained" variability in the $y$ values,  
and thus $R^2 = 1- \frac{s_{\textrm{residuls}}^2}{s_y^2}$ is the proportion of "explained" variability in the $y$ values 




## $R^2$ = Coefficient of determination (2/2)

+ Recall, $-1<r<1$

+ Thus, $0<R^2<1$

+ In practice, we want "high" $R^2$ values, i.e. $R^2$ as close to 1 as possible.

Calculating $R^2$ in R using `glance()` from the `broom` package:
```{r}
glance(model1)
glance(model1)$r.squared
```

:::{.callout-warning}
* A model can have a high $R^2$ value when there is a curved pattern.
* Always first check whether a linear model is reasonable or not.
:::



## $R^2$ in `summary()` R output

```{r}
summary(model1)
```

Compare to the square of the correlation coefficient $r$:

```{r}
r <- cor(x = gapm$life_expectancy_years_2011, 
    y = gapm$female_literacy_rate_2011,
    use =  "complete.obs")
r
r^2
```


# Inference for intercept and slope


```{r}
# Fit regression model:
model1 <- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,
                 data = gapm)
# Get regression table:
tidy(model1, conf.int = TRUE) %>% gt() # conf.int = TRUE part is new! 
```


\begin{align}
\widehat{y} =& b_0 + b_1 \cdot x\\
\widehat{\text{life expectancy}} =& 50.9 + 0.232 \cdot \text{female literacy rate}
\end{align}


+ What are $H_0$ and $H_A$?
+ How do we calculate the standard error, statistic, _p_-value, and CI?



## Inference for the population [__slope__]{style="color:green"}: CI and hypothesis test

::: columns
::: {.column width="50%"}
__Population model__  
_line + random "noise"_

$$y = \beta_0 + \beta_1 \cdot x + \varepsilon$$
with $\varepsilon \sim N(0,\sigma)$  
$\sigma$ is the variability (SD) of the residuals

<br>

__Sample best-fit (least-squares) line:__

$$\widehat{y} = b_0 + b_1 \cdot x $$

Note: Some sources use $\widehat{\beta}$ instead of $b$. 
:::

::: {.column width="50%"}

+ Construct a __95% confidence interval__ for the __population slope__ $\beta_1$

<br>

+ Conduct the __hypothesis test__ 

\begin{align}
H_0 &: \beta_1 = 0\\
\text{vs. } H_A&: \beta_1 \neq 0
\end{align}

<br>

_Note: R reports p-values for 2-sided tests_

:::
:::



## CI for population [__slope__]{style="color:green"} $\beta_1$

Recall the general CI formula: 

$$\textrm{Point Estimate} \pm t^*\cdot SE_{\textrm{Point Estimate}}$$

For the CI of the coefficient $b_1$ this translates to

$$b_1 \pm t^*\cdot SE_{b_1}$$
where $t^*$ is the critical value from a $t$-distribution with $df = n -2$.

<br>

_How is_ $\text{SE}_{b_1}$ _calculated?_ See next slide.

<br>

```{r}
tidy(model1, conf.int = TRUE)
```


## Standard error of fitted slope $b_1$

::: columns
::: {.column width="50%"}
$$\text{SE}_{b_1} = \frac{s_{\textrm{residuals}}}{s_x\sqrt{n-1}}$$

:::


::: {.column width="50%"}
$\text{SE}_{b_1}$ is the __variability__ of the statistic $b_1$
:::

:::

::: {style="font-size: 90%;"}
::: columns
::: {.column width="30%"}
+ $s_{\textrm{residuals}}^2$ is the sd of the residuals
:::
::: {.column width="1%"}
:::

::: {.column width="30%"}
+ $s_x$ is the sample sd of the explanatory variable $x$
:::
::: {.column width="1%"}
:::

::: {.column width="30%"}
+ $n$ is the sample size, or the number of (complete) pairs of points
:::
:::
:::

```{r}
glance(model1)

# standard deviation of the residuals
(s_resid <- glance(model1)$sigma)

# standard deviation of x's
(s_x <- sd(gapm$female_literacy_rate_2011))

# number of pairs of complete observations
(n <- nobs(model1))

(se_b1 <- s_resid/(s_x * sqrt(n-1))) # compare to SE in regression output
```

## Calculate CI for population [__slope__]{style="color:green"} $\beta_1$


::: columns
::: {.column width="50%"}
$$b_1 \pm t^*\cdot SE_{b_1}$$  
:::

::: {.column width="50%"}
where $t^*$ is the $t$-distribution critical value with $df = n -2$.

:::
:::

```{r}
tidy(model1, conf.int = TRUE) %>% gt()
```

Save regression output for the row with the slope's information: 

```{r}
model1_b1 <-tidy(model1) %>% filter(term == "female_literacy_rate_2011")
model1_b1 %>% gt()
```


::: columns
::: {.column width="50%"}

Save values needed for CI:

```{r}
b1 <- model1_b1$estimate
SE_b1 <- model1_b1$std.error
```


```{r}
nobs(model1) # sample size n
(tstar <- qt(.975, df = 80-2))
```

:::

::: {.column width="50%"}
Compare CI bounds below with the ones in the regression table above.

```{r}
(CI_LB <- b1 - tstar*SE_b1)
(CI_UB <- b1 + tstar*SE_b1)
```
:::
:::


## Hypothesis test for population [__slope__]{style="color:green"} $\beta_1$


::: columns
::: {.column width="40%"}
\begin{align}
H_0 &: \beta_1 = 0\\
\text{vs. } H_A&: \beta_1 \neq 0
\end{align}


:::

::: {.column width="60%"}

The __test statistic__ for $b_1$ is 

$$t = \frac{ b_1 - \beta_1}{ \text{SE}_{b_1}} = \frac{ b_1}{ \text{SE}_{b_1}}$$

when we assume $H_0: \beta_1 = 0$ is true.
:::
:::

```{r}
tidy(model1, conf.int = TRUE) %>% gt()
```

Calculate the test statistic using the values in the regression table:

```{r}
# recall model1_b1 is regression table restricted to b1 row
(TestStat <- model1_b1$estimate / model1_b1$std.error)
```

Compare this test statistic value to the one from the regression table above



## $p$-value for testing population [__slope__]{style="color:green"} $\beta_1$

* As usual, the $p$-value is the _probability of obtaining a test statistic_ __just as extreme or more extreme__ _than the observed test statistic assuming the null hypothesis_ $H_0$ _is true._

* To calculate the $p$-value, we need to know the probability distribution of the test statistic (the _null distribution_) assuming $H_0$ is true.

+ Statistical theory tells us that the test statistic $t$ can be modeled by a [__$t$-distribution__]{style="color:green"} with [__$df = n-2$__]{style="color:green"}.

+ Recall that this is a 2-sided test:

```{r}
(pv = 2*pt(TestStat, df=80-2, lower.tail=F))
```

Compare the $p$-value to the one from the regression table below

```{r}
tidy(model1, conf.int = TRUE) %>% gt()  # compare p-value calculated above to p-value in table
```




# Caution...

## Corrrelation doesn't imply causation`*`!


* This might seem obvious, but make sure to not write your analysis results in a way that implies causation if the study design doesn't warrant it (such as an observational study).

* Beware of spurious correlations: <http://www.tylervigen.com/spurious-correlations>

![](/img_slides/spurious_corr_2.png)

* `*`Caveat: there is a whole field of statistics/epidemiology on causal inference. <https://ftp.cs.ucla.edu/pub/stat_ser/r350.pdf>


## What's next?

<br>
<br>

![](/img_slides/flowchart_only_continuous.jpg){fig-align="center"}






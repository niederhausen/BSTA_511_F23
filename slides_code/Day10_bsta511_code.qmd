---
title: "DRAFT Day 10: Hypothesis testing for one-sample mean (4.3, 5.1) & two-sample paired data mean (5.2)"
subtitle: "BSTA 511/611"
author: "Meike Niederhausen, PhD"
institute: "OHSU-PSU School of Public Health"
date: "11/1/2023"
categories: ["Week 6"]
format: 
  html:
    link-external-newwindow: true
    toc: true
execute:
  echo: true
# editor: visual
editor_options: 
  chunk_output_type: console
---

```{r}
#| label: "setup"
#| include: false
knitr::opts_chunk$set(echo = TRUE, fig.height=3, fig.width=5, message = F)
```

## Load packages

* Packages need to be loaded _every time_ you restart R or render an Qmd file

```{r}
# run these every time you open Rstudio
library(tidyverse)    
library(oibiostat)
library(janitor)
library(rstatix)
library(knitr)
library(gtsummary)
library(moderndive)
library(gt)
library(broom) # NEW!!

set.seed(456)
```


- You can check whether a package has been loaded or not 
  - by looking at the Packages tab and 
  - seeing whether it has been checked off or not


## Goals for today (Sections 4.3 & 5.1)

### Hypothesis testing for one-sample mean (4.3, 5.1) 

* Introduce hypothesis testing using the case of analyzing a mean from one sample (group)

* Steps of a hypothesis test:
    1. level of significance
    1. null ( $H_0$ ) and alternative ( $H_A$ ) hypotheses
    1. test statistic
    1. p-value
    1. conclusion

* Run a t-test in R, and `tidy()` the test output using `broom` package

### Inference for two-sample paired data mean (5.2)

* Confidence intervals (CIs) and hypothesis testing for the mean of two-sample dependent/paired data

### One-sided CIs

### CIs vs. hypothesis tests (4.3.3)



# Is 98.6°F  really the mean "healthy" body temperature?

* __Where did the 98.6°F value come from?__
    * German physician Carl Reinhold August [Wunderlich](https://www.google.com/books/edition/_/a6UNq33GPfIC?hl=en&gbpv=1&pg=PP14) determined  98.6°F (or 37°C) based on temperatures from 25,000 patients in Leipzig in 1851.

* [1992 JAMA article](https://jamanetwork.com/journals/jama/article-abstract/400116) by Mackowiak, Wasserman, & Levine
    * They claim that 98.2°F (36.8°C) is a more accurate average body temp
    * Sample: n = 148 healthy men and women aged 18 - 40 years

* In January 2020, a group from Stanford published _[Decreasing human body temperature in the United States since the Industrial Revolution](https://elifesciences.org/articles/49555)_ in eLIFE.
    * "determined that mean body temperature in men and women, after adjusting for age, height, weight and, in some models date and time of day, has decreased monotonically by 0.03°C (0.05°F) per birth decade"
    * September 2023 update: _[Defining Usual Oral Temperature Ranges in Outpatients Using an Unsupervised Learning Algorithm](https://jamanetwork.com/journals/jamainternalmedicine/article-abstract/2809098)_ in JAMA Internal Medicine
        * Average is 36.64 °C (97.95 °F); "range of mean temperatures for the coolest to the warmest individuals was 36.24 °C to 36.89 °C" (97.23 to 98.40 °F); based 2008-2017 data
        * "findings suggest that age, sex, height, weight, and time of day are factors that contribute to variations in individualized normal temperature ranges."

* NYT article [The Average Human Body Temperature Is Not 98.6 Degrees](../resources/NYT_What_Is_a_Fever_Why_Your_Body_Temperature_May_Be_Cooler_Than_98.6_Degrees.pdf), Oct 12, 2023, by Dana G. Smith

__[Question:]{style="color:darkorange"} based on the 1992 JAMA data, is there evidence to support that the population mean body temperature is different from 98.6°F?__



## Question: based on the 1992 JAMA data, is there evidence to support that the population mean body temperature is different from 98.6°F?


Two approaches to answer this question:

1. Create a __confidence interval (CI)__ for the population mean $\mu$ and determine whether 98.6°F is inside the CI or not.
    * is 98.6°F a plausible value?

2. Run a __hypothesis test__ to see if there is evidence that the population mean $\mu$ is _significantly different_ from 98.6°F or not.
    * This does not give us a range of plausible values for the population mean $\mu$.
    
    * Instead, we calculate a _test statistic_ and _p-value_ 
        * to see how likely we are to observe the sample mean $\bar{x}$
        * or a more extreme sample mean 
        * assuming that the population mean $\mu$ is 98.6°F.



## Approach 1: Create a 95% CI for the population mean body temperature

* Use data based on the results from the 1992 JAMA study
    * The original dataset used in the JAMA article is not available
    * However, Allen Shoemaker from Calvin College created a [dataset](http://jse.amstat.org/datasets/normtemp.dat.txt) with the same summary statistics as in the JAMA article, which we will use:

$$\bar{x} = 98.25,~s=0.733,~n=130$$
CI for $\mu$:

```{r}
#| include: false
n <- 130
xbar <- 98.25
sd <- 0.733
(tstar <- qt(.975, df=n-1))  # df = n-1
(se <- sd/sqrt(n))
(moe <- tstar * se) 
(LB <- xbar - moe)
(UB <- xbar + moe)
```


$$\bar{x} \pm t^*\cdot\frac{s}{\sqrt{n}}\\
98.25 \pm `r round(tstar,3)`\cdot\frac{0.733}{\sqrt{130}}\\
98.25 \pm `r round(moe,3)`\\
(`r round(LB, 3)`, `r round(UB, 3)`)$$




Used $t^*$ = `qt(.975, df=129)` 

Conclusion:  
We are 95% that the (population) mean body temperature is between `r round(LB, 3)`°F and `r round(UB, 3)`°F.

* _How does the CI compare to 98.6°F?_





## Approach 2: Hypothesis Test 

From before: 

* Run a __hypothesis test__ to see if there is evidence that the population mean $\mu$ is _significantly different_ from 98.6°F or not.
    * This does not give us a range of plausible values for the population mean $\mu$.
    
    * Instead, we calculate a _test statistic_ and _p-value_ 
        * to see how likely we are to observe the sample mean $\bar{x}$
        * or a more extreme sample mean 
        * assuming that the population mean $\mu$ is 98.6°F.

__How do we calculate a _test statistic_ and _p-value_?__




## Recall the sampling distribution of the mean

From the __Central Limit Theorem (CLT)__, we know that

* For **"large" sample sizes** ( $n\geq 30$ ),
    * the __sampling distribution__ of the sample mean
    * can be approximated by a __normal distribution__,with 
      * _mean_ equal to the _population mean_ value $\mu$, and 
      * _standard deviation_ $\frac{\sigma}{\sqrt{n}}$

$$\bar{X}\sim N\Big(\mu_{\bar{X}} = \mu, \sigma_{\bar{X}}= \frac{\sigma}{\sqrt{n}}\Big)$$

* For **small sample sizes**, if the population is known to be normally distributed, then
    * the __sampling distribution__ of the sample mean
    * is a __normal distribution__,with 
      * _mean_ equal to the _population mean_ value $\mu$, and 
      * _standard deviation_ $\frac{\sigma}{\sqrt{n}}$



### Case 1: suppose we know the population sd $\sigma$

* [How likely we are to observe the sample mean $\bar{x}$
      * or a more extreme sample mean 
      * assuming that the population mean $\mu$ is 98.6°F?]{style="color:green"}
* Use $\bar{x} = 98.25$, $\sigma=0.733$, and $n=130$


```{r}
#| fig.width: 6
#| fig.height: 3
#| echo: false
# specify upper and lower bounds of shaded region below
mu <- 98.6
std <- 0.06428835

ggplot(data.frame(x = c(mu-4*std, mu+4*std)), aes(x = x)) + 
  stat_function(fun = dnorm, 
                args = list(mean = mu, sd = std)) + 
  # stat_function(fun = dnorm, 
  #               args = list(mean = mu, sd = std), 
  #         # specify the upper and lower bounds of the shaded region:
  #               xlim = c(mu-4*std, xbar),             
  #               geom = "area", fill = "darkblue") +
  # the breaks values below might need to be adjusted 
  # if there are too many values showing on the x-axis
  # scale_x_continuous(breaks=(mu-4*std):(mu+4*std)) +
  scale_y_continuous(breaks = NULL) +
  scale_x_continuous(breaks=c(mu, mu - 0.06*(1:5), mu + 0.06*(1:5))) +
  theme(axis.text.x=element_text(angle = -30, hjust = 0)) +
  labs(y = "", 
       x = "sample mean",
       title = "Sampling distribution of mean body temperatures") 
```


### Case 2: we don't know the population sd $\sigma$ 


* This is usually the case in real life
* We estimate $\sigma$ with the sample standard deviation $s$
* From last time, we know that in this case we need to use the __t-distribution with d.f. = n-1__, instead of the normal distribution 
* [Question: How likely we are to observe the sample mean $\bar{x}$ or a more extreme sample mean, assuming that the population mean $\mu$ is 98.6°F?]{style="color:green"}
* Use $\bar{x} = 98.25$, $s=0.733$, and $n=130$


# Steps in a Hypothesis Test


1. Set the __[level of significance]{style="color:darkorange"}__ $\alpha$

1. Specify the __[null]{style="color:darkorange"}__ ( $H_0$ ) and __[alternative]{style="color:darkorange"}__ ( $H_A$ ) __[hypotheses]{style="color:darkorange"}__
    1. In symbols
    1. In words
    1. Alternative: one- or two-sided?

1. Calculate the __[test statistic]{style="color:darkorange"}__. 

1. Calculate the __[p-value]{style="color:darkorange"}__ based on the observed test statistic and its sampling distribution

1. Write a __[conclusion]{style="color:darkorange"}__ to the hypothesis test
    1. Do we reject or fail to reject $H_0$?
    1. Write a conclusion in the context of the problem



## Step 2: Null & Alternative Hypotheses 

In statistics, a __hypothesis__ is a statement about the value of an __unknown population parameter__.

A __[hypothesis test]{style="color:darkorange"}__ consists of a test between two competing hypotheses: 

1. a __[null]{style="color:darkorange"}__ hypothesis $H_0$ (pronounced “H-naught”) vs. 
1. an __[alternative]{style="color:darkorange"}__ hypothesis $H_A$ (also denoted $H_1$)


\begin{aligned}
H_0 &: \text{The population mean body temperature is 98.6°F}\\
\text{vs. } H_A &: \text{The population mean body temperature is not 98.6°F}
\end{aligned}

1. the null hypothesis $H_0$ is a claim that there is “no effect” or “no difference of interest.”
1. the alternative hypothesis $H_A$ is the claim a researcher wants to establish or find evidence to support. It is viewed as a “challenger” hypothesis to the null hypothesis $H_0$

\begin{aligned}
H_0 &: \mu = 98.6\\
\text{vs. } H_A&: \mu \neq 98.6
\end{aligned}



## Step 3: Test statistic (& its distribution)

__[Case 1: know population sd $\sigma$]{style="color:darkorange"}__

$$
\text{test statistic} = z_{\bar{x}} = \frac{\bar{x} - \mu_0}{\frac{\sigma}{\sqrt{n}}}
$$
* Statistical theory tells us that $z_{\bar{x}}$ follows a __Standard Normal distribution__ $N(0,1)$.


__[Case 2: don't know population sd $\sigma$]{style="color:darkorange"}__

$$
\text{test statistic} = t_{\bar{x}} = \frac{\bar{x} - \mu_0}{\frac{s}{\sqrt{n}}}
$$

* Statistical theory tells us that $t_{\bar{x}}$ follows a __Student's t distribution__ with degrees of freedom (df) = $n-1$.



$\bar{x}$ = sample mean, $\mu_0$ = hypothesized population mean from $H_0$,  
$\sigma$ = _population_ standard deviation, $s$ = _sample_ standard deviation, $n$ = sample size

__[Assumptions]{style="color:darkorange"}__:

* __Independent observations__ 
    * The observations were collected independently.
* __Approximately normal sample or big n__ 
    * The distribution of the sample should be approximately normal 
    * _or_ the sample size should be at least 30.



## Step 4: p-value

The __[p-value]{style="color:darkorange"}__ is the __probability__ of obtaining a test statistic _just as extreme or more extreme_ than the observed test statistic assuming the null hypothesis $H_0$ is true. 


* The $p$-value is a quantification of "surprise"
    * Assuming $H_0$ is true, _how surprised are we with the observed results_?
    * _Ex_: assuming that the true mean body temperature is 98.6°F, how surprised are we to get a sample mean of 98.25°F  (or more extreme)?
    
* If the $p$-value is "small," it means there's a small probability that we would get the observed statistic (or more extreme) when $H_0$ is true.



```{r}
#| fig.width: 6
#| fig.height: 3
#| echo: false
# specify upper and lower bounds of shaded region below
mu <- 98.6
std <- 0.06428835

ggplot(data.frame(x = c(mu-4*std, mu+4*std)), aes(x = x)) + 
  stat_function(fun = dnorm, 
                args = list(mean = mu, sd = std)) + 
  # stat_function(fun = dnorm, 
  #               args = list(mean = mu, sd = std), 
  #         # specify the upper and lower bounds of the shaded region:
  #               xlim = c(mu-4*std, xbar),             
  #               geom = "area", fill = "darkblue") +
  # the breaks values below might need to be adjusted 
  # if there are too many values showing on the x-axis
  # scale_x_continuous(breaks=(mu-4*std):(mu+4*std)) +
  scale_y_continuous(breaks = NULL) +
  scale_x_continuous(breaks=c(mu, mu - 0.06*(1:5), mu + 0.06*(1:5))) +
  theme(axis.text.x=element_text(angle = -30, hjust = 0)) +
  labs(y = "", 
       x = "sample mean",
       title = "Sampling distribution of mean body temperatures") 
```


## Step 4: p-value calculation 


First we need the __test statistic__:

Recall that $\bar{x} = 98.25$, $s=0.733$, and $n=130.$

$$t_{\bar{x}} = \frac{\bar{x} - \mu_0}{\frac{s}{\sqrt{n}}}
= t_{\bar{x}} = \frac{98.25 - 98.6}{\frac{0.73}{\sqrt{130}}}
= -5.45$$

* Statistical theory tells us that $t_{\bar{x}}$ follows a __student's t-distribution__ with $d.f. = n-1 = 129$.


* __Assumptions met?__



```{r}
#| fig.width: 6
#| fig.height: 3
#| echo: false
ggplot(data = data.frame(x = c(-6, 6)), aes(x)) + 
  stat_function(fun = dt, args = list(df = 129)) + 
  ylab("") + 
  xlab("t-dist with df = 129") +
  scale_y_continuous(breaks = NULL) + 
  geom_vline(xintercept = c(-5.45,5.45), 
             color = "red")
```


Calculate the _p_-value using the t-distribution:

$$p-value=P(t \leq -5.45) + P(t \geq 5.45)$$
```{r}
2*pt(-5.4548, df = 130-1, 
     lower.tail = TRUE)
```  



## Step 1: Significance Level $\alpha$

* Before doing a hypothesis test, we set a cut-off for    how small the $p$-value should be in order to reject $H_0$.
* We call this the __significance level__.
* We use the Greek symbol alpha ( $\alpha$ ) to denote the significance level
* Typical $\alpha$ values are 
    * 0.05 - _most common by far!!_
    * 0.01 and 0.1
    
* When $p$-value < $\alpha$, we "__reject__ the null hypothesis $H_0$."
* When $p$-value $\geq \alpha$, we "__fail to reject__ the null hypothesis $H_0$."

* __[Important:]{style="color:darkred"}__ "failing to reject" $H_0$ is __not__ the same as "accepting" $H_0$! 
    * By failing to reject $H_0$ we are just saying that we don't have sufficient evidence to support the alternative $H_A$.




## Step 5: Conclusion to hypothesis test

$$H_0: \mu = 98.6\\
\text{vs. } H_A: \mu \neq 98.6$$

* Recall the $p$-value = $2.41\cdot 10^{-7}$ 
* Use $\alpha$ = 0.05.
* Do we reject or fail to reject $H_0$?

__Conclusion statement__:

* Basic: 
    * There is sufficient evidence that the (population) mean body temperature is different from 98.6°F ( $p$-value < 0.001).

* Better: 
    * The average body temperature in the sample was 98.25°F (95% CI 98.12 to 98.38°F), and there is sufficient evidence that the (population) mean body temperature is different from 98.6°F ( $p$-value < 0.001).




# `t.test`: R's built-in command for testing one mean 

Using the body temperature example with $H_A: \mu \neq 98.6$

```{r}
BodyTemps <- read_csv("../data/BodyTemperatures.csv")

(temps_ttest <- t.test(x = BodyTemps$Temperature,
       # alternative = "two.sided",  # default
       mu = 98.6))
```

Note that the test output also gives the 95% CI using the t-distribution.




## `tidy` the `t.test` output


```{r}
# use tidy command from broom package for briefer output that's a tibble
tidy(temps_ttest) %>% 
  gt()
```

* The `tidy()` output is a tibble, and so we can pull specific values from it:



```{r}
# base R
tidy(temps_ttest)$p.value  
```




```{r}
# or with tidyverse:
tidy(temps_ttest) %>% pull(p.value)
```




# MoRitz's tip of the day

Use R projects (.Rproj) to organize your code and data files.

See [file on Sakai](https://sakai.ohsu.edu/access/content/group/BSTA-511-OL-AV-F22/miscellaneous/Projects_in_R.html) with more information.


# What's next? 

CI's and hypothesis testing for different scenarios:

Section | Population parameter | Symbol | Point estimate | Symbol 
--------|--------|--------|--------|--------
5.1 | Pop mean | $\mu$ | Sample mean | $\bar{x}$ 
5.2 | Diff in paired pop means | $\delta$ | Diff in paired sample means | $\bar{x}_{diff}$ 
5.3 | Diff in pop means | $\mu_1-\mu_2$ | Diff in sample means | $\bar{x}_1 - \bar{x}_2$ 
8.1 | Pop proportion | $p$ | Sample prop | $\widehat{p}$ 
8.2 | Diff in pop prop's | $p_1-p_2$ | Diff in sample prop's | $\widehat{p}_1-\widehat{p}_2$

# Paired data


# Where are we?

CI's and hypothesis tests for different scenarios:

$$point~estimate \pm z^*(or~t^*)\cdot SE,~~~~~~~~test~stat = \frac{point~estimate-null~value}{SE}$$

Day | Book | Population <br> parameter | Symbol | Point estimate | Symbol | SE
--------|--------|--------|--------|--------|--------|--------
11-12 | 5.1 | Pop mean | $\mu$ | Sample mean | $\bar{x}$ | $\frac{s}{\sqrt{n}}$
13 | 5.2 | Diff in paired <br> pop means | $\mu_d$ or $\delta$ | Diff in paired <br> sample means | $\bar{x}_{d}$  |
14 | 5.3 | Diff in pop <br> means | $\mu_1-\mu_2$ | Diff in sample <br> means | $\bar{x}_1 - \bar{x}_2$  |
 | 8.1 | Pop proportion | $p$ | Sample prop | $\widehat{p}$  |
 | 8.2 | Diff in pop <br> proportions | $p_1-p_2$ | Diff in sample <br> proportions | $\widehat{p}_1-\widehat{p}_2$ |



## Steps in a Hypothesis Test


1. Set the __[level of significance]{style="color:darkorange"}__ $\alpha$

1. Specify the __[null]{style="color:darkorange"}__ ( $H_0$ ) and __[alternative]{style="color:darkorange"}__ ( $H_A$ ) __[hypotheses]{style="color:darkorange"}__
    1. In symbols
    1. In words
    1. Alternative: one- or two-sided?

1. Calculate the __[test statistic]{style="color:darkorange"}__. 

1. Calculate the __[p-value]{style="color:darkorange"}__ based on the observed test statistic and its sampling distribution

1. Write a __[conclusion]{style="color:darkorange"}__ to the hypothesis test
    1. Do we reject or fail to reject $H_0$?
    1. Write a conclusion in the context of the problem


# Goals for today (Section 5.2)

* Statistical inference for paired data (2 samples)
    1. What are paired data?  
    1. EDA of data
    1. Hypothesis test
    1. Confidence Interval
    1. Run test in R


## Examples of paired designs (two samples)

* Enroll pairs of identical twins to study a disease
* Enroll father & son pairs to study cholesterol levels
* Studying pairs of eyes
* Enroll people and collect data before & after an intervention (longitudinal data)
* Book: Compare maximal speed of competitive swimmers wearing a wetsuit vs. wearing a regular swimsuit


# Can a vegetarian diet change cholesterol levels?

* __Scenario__:
  * 24 non-vegetarian people were enrolled in a study
  * They were instructed to adopt a vegetarian diet
  * Cholesterol levels were measured before and after the diet
* __Question__: Is there evidence to support that cholesterol levels changed after the vegetarian diet?
* How to answer the question?
  * First, calculate changes (differences) in cholesterol levels
      * We usually do after - before if the data are longitudinal


Calculate __CI for the mean difference__ $\delta$:

$$\bar{x}_d \pm t^*\cdot\frac{s_d}{\sqrt{n}}$$

Run a __hypothesis test__

Hypotheses

$$H_0: \delta = \delta_0 \\
H_A: \delta \neq \delta_0 \\
(or~ <, >)$$


Test statistic

$$
t_{\bar{x}_d} = \frac{\bar{x}_d - \delta_0}{\frac{s_d}{\sqrt{n}}}
$$


## EDA: Explore the cholesterol data

* Scenario:
  * 24 non-vegetarian people were enrolled in a study
  * They were instructed to adopt a vegetarian diet
  * Cholesterol levels were measured before and after the diet

```{r}
chol <- read_csv("../data/chol213.csv")
glimpse(chol)
chol %>% get_summary_stats(type = "common") %>% gt()
```


### EDA: Cholesterol levels before and after vegetarian diet


```{r }
ggplot(chol, aes(x=Before)) +
  geom_density()
ggplot(chol, aes(x=Before)) +
  geom_boxplot()
```

```{r }
ggplot(chol, aes(x=After)) +
  geom_density()
ggplot(chol, aes(x=After)) +
  geom_boxplot()
```


### EDA: Spaghetti plot of cholesterol levels before & after diet

* Visualize the individual before vs. after diet changes in cholesterol levels

```{r}
#| fig.height: 4
chol_long <- chol %>% 
  # need an ID column for the plot
  # make it factor so that coloring is not on continuous scale
  mutate(ID = factor(1:n())) %>% 
  # make data long for plot: 
  pivot_longer(
    cols = Before:After,
    names_to = "Time",  # need a column for Before & After on x-axis
    values_to = "Cholesterol") %>% # need a column of all cholesterol values for y-axis
  mutate(
    # change Time a factor variable so that can reorder
    # levels so that Before is before After
    Time = factor(Time, levels = c("Before", "After"))
    )
  
ggplot(chol_long, 
       aes(x=Time, y = Cholesterol, 
           # need to include group = ID 
           # to create a line for each ID
           color = ID, group = ID)) + 
  geom_line(show.legend = FALSE)
```


### EDA: Differences in cholesterol levels: After - Before diet


```{r }
chol <- chol %>% 
  mutate(DiffChol = After-Before) 
head(chol, 12)

```

```{r }
chol %>% 
  get_summary_stats(type = "common") %>% 
  gt()
```


### EDA: Differences in cholesterol levels: After - Before diet

```{r }
ggplot(chol, aes(x=Before)) +
  geom_density()
ggplot(chol, aes(x=After)) +
  geom_density()
```

```{r }
ggplot(chol, aes(x=DiffChol)) + 
  geom_density()
ggplot(chol, aes(x=DiffChol)) + 
  geom_boxplot()
```


# Steps in a Hypothesis Test


1. Set the __[level of significance]{style="color:darkorange"}__ $\alpha$

1. Specify the __[null]{style="color:darkorange"}__ ( $H_0$ ) and __[alternative]{style="color:darkorange"}__ ( $H_A$ ) __[hypotheses]{style="color:darkorange"}__
    1. In symbols
    1. In words
    1. Alternative: one- or two-sided?

1. Calculate the __[test statistic]{style="color:darkorange"}__. 

1. Calculate the __[p-value]{style="color:darkorange"}__ based on the observed test statistic and its sampling distribution

1. Write a __[conclusion]{style="color:darkorange"}__ to the hypothesis test
    1. Do we reject or fail to reject $H_0$?
    1. Write a conclusion in the context of the problem


## Step 2: Null & Alternative Hypotheses 

* __Question__: Is there evidence to support that cholesterol levels changed after the vegetarian diet?


Null and alternative hypotheses in __words__
Include as much context as possible

<br>

* $H_0$: The population mean difference in cholesterol levels after a vegetarian diet is  

* $H_A$: The population mean difference in cholesterol levels after a vegetarian diet is



Null and alternative hypotheses in __symbols__

$$~~~~H_0: \delta =  \\
H_A: \delta   \\$$


## Step 3: Test statistic 

```{r}
chol %>% select(DiffChol) %>% get_summary_stats(type = "common") %>% gt()
```

$$
t_{\bar{x}_d} = \frac{\bar{x}_d - \delta_0}{\frac{s_d}{\sqrt{n}}}
$$

* Based on the value of the test statistic, do you think we are going to reject or fail to reject $H_0$?
* What probability distribution does the test statistic have?
* Are the __[assumptions]{style="color:darkorange"}__ for a paired t-test satisfied so that we can use the probability distribution to calculate the $p$-value??


```{r}
n <- 24
alpha <- 0.05
mu <- 0
(p_area <- 1-alpha/2)
(xbar <- mean(chol$DiffChol))
(sd <- sd(chol$DiffChol))
(se <- sd/sqrt(n))

(tstat <- (xbar - mu)/se)
```



## Step 4: p-value

The __[p-value]{style="color:darkorange"}__ is the __probability__ of obtaining a test statistic _just as extreme or more extreme_ than the observed test statistic assuming the null hypothesis $H_0$ is true. 


```{r}
# specify upper and lower bounds of shaded region below
mu <- 0
std <- se

# The following figure is only an approximation of the 
# sampling distribution since I used a normal instead
# of t-distribution to make it.

ggplot(data.frame(x = c(mu-6*std, mu+6*std)), aes(x = x)) + 
  stat_function(fun = dnorm, 
                args = list(mean = mu, sd = std)) + 
  scale_y_continuous(breaks = NULL) +
  scale_x_continuous(breaks=c(mu, mu - 3.4*(1:6), mu + 3.4*(1:6))) +
  theme(axis.text.x=element_text(angle = -30, hjust = 0)) +
  labs(y = "", 
       x = "sample mean difference",
       title = "Sampling distribution of mean difference") +
  geom_vline(xintercept = c(-xbar, xbar), 
             color = "red")
```

```{r}
ggplot(data = data.frame(x = c(-6, 6)), aes(x)) + 
  stat_function(fun = dt, args = list(df = n-1)) + 
  ylab("") + 
  xlab("t-dist with df = 23") +
  scale_y_continuous(breaks = NULL) + 
  scale_x_continuous(breaks=c(mu, mu - (1:5), mu + (1:5))) +
  geom_vline(xintercept = c(-tstat,tstat), 
             color = "red")
```


Calculate the _p_-value:

```{r}
(pv <- 2*(pt(tstat, df=n-1)))
```  


## Step 5: Conclusion to hypothesis test

$$~~~~H_0: \delta = 0 \\
H_A: \delta \neq 0  \\$$

* Recall the $p$-value = $8.434775 \cdot 10 ^{-6}$
* Use $\alpha$ = 0.05.
* Do we reject or fail to reject $H_0$?

__Conclusion statement__:

* Stats class conclusion
    * There is sufficient evidence that the (population) mean difference in cholesterol levels after a vegetarian diet is different from 0 mg/dL ( $p$-value < 0.001).

* More realistic manuscript conclusion: 
    * After a vegetarian diet, cholesterol levels decreased by on average 19.54 mg/dL (SE = 3.43 mg/dL, 2-sided $p$-value < 0.001).



# 95% CI for the mean difference in cholesterol levels

```{r}
chol %>% select(DiffChol) %>% get_summary_stats(type = "common") %>% gt()
```

CI for $\mu_d$ (or $\delta$):

```{r}
n <- 24
alpha <- 0.05
(p_area <- 1-alpha/2)
(xbar <- mean(chol$DiffChol))
(sd <- sd(chol$DiffChol))
(tstar <- qt(p_area, df=n-1))  # df = n-1
(se <- sd/sqrt(n))
(moe <- tstar * se) 
(LB <- xbar - moe)
(UB <- xbar + moe)
```


$$\bar{x}_d \pm t^*\cdot\frac{s_d}{\sqrt{n}}\\
`r round(xbar,3)` \pm `r round(tstar,3)`\cdot\frac{`r round(sd,3)`}{\sqrt{`r n`}}\\
`r round(xbar,3)` \pm `r round(tstar,3)`\cdot `r round(se,3)`\\
`r round(xbar,3)` \pm `r round(moe,3)`\\
(`r round(LB, 3)`, `r round(UB, 3)`)$$


Conclusion:  
We are 95% that the (population) mean difference in cholesterol levels after a vegetarian diet is between `r round(LB, 3)` mg/dL and `r round(UB, 3)` mg/dL.

* _Based on the CI, is there evidence the diet made a difference in cholesterol levels? Why or why not?_

# R

## R option 1: Run a 1-sample `t.test` using the paired differences

$H_A: \delta \neq 0$

```{r}
t.test(x = chol$DiffChol, mu = 0)
```



## R option 2: Run a 2-sample `t.test` with `paired = TRUE` option

$H_A: \delta \neq 0$

* For a 2-sample t-test we specify both `x=` and `y=`
* Note: `mu = 0` is the default value and doesn't need to be specified

```{r}
t.test(x = chol$Before, y = chol$After, mu = 0, paired = TRUE)
```



## R option 3: Run a 2-sample `t.test` with `paired = TRUE` option, but using the long data and a "formula"


* Use the usual `t.test`
* What's different is that 
    * instead of specifying the variables with `x=` and `y=`, 
    * we give a __formula__ of the form `y ~ x` using _just the variable names_,
    * and then specify the name of the dataset using `data =`
* This method is often used in practice, and more similar to the coding style of running a regression model (BSTA 512 & 513)



```{r}
# using long data with columns Cholesterol & Time:
t.test(Cholesterol ~ Time, 
       paired = TRUE, 
       data = chol_long)
```



## `tidy` the `t.test` output & compare the 3 options


```{r}
# option 1
t.test(x = chol$DiffChol, mu = 0) %>% tidy() %>% gt() # tidy from broom package
# option 2
t.test(x = chol$Before, y = chol$After, mu = 0, paired = TRUE) %>% tidy() %>% gt()
# option 3
t.test(Cholesterol ~ Time, paired = TRUE, data = chol_long) %>% tidy() %>% gt()
```



# What if we wanted to test whether the diet _decreased_ cholesterol levels?


How are the steps different?

1. Set the __[level of significance]{style="color:darkorange"}__ $\alpha$

1. Specify the __[hypotheses]{style="color:darkorange"}__ $H_0$ and $H_A$
    * Alternative: one- or two-sided?

1. Calculate the __[test statistic]{style="color:darkorange"}__. 

1. Calculate the __[p-value]{style="color:darkorange"}__ based on the observed test statistic and its sampling distribution

1. Write a __[conclusion]{style="color:darkorange"}__ to the hypothesis test


## R: What if we wanted to test whether the diet _decreased_ cholesterol levels?

```{r}
# alternative = c("two.sided", "less", "greater")
t.test(x = chol$DiffChol, mu = 0, alternative = "less") %>% tidy() %>% gt()
```



# One-sided confidence intervals


Formula for a __2-sided__ (1- $\alpha$ )% __CI__:

$$\bar{x} \pm t^*\cdot\frac{s}{\sqrt{n}}$$
* $t^*$ = `qt(1-alpha/2, df = n-1)`
* $\alpha$ is split over both tails of the distribution


A __one-sided__ (1- $\alpha$ )% __CI__ has all (1- $\alpha$ )% on just the left or the right tail of the distribution:

$$(\bar{x} - t^*\cdot\frac{s}{\sqrt{n}},~\infty) \\
(\infty,~\bar{x} + t^*\cdot\frac{s}{\sqrt{n}})$$

* $t^*$ = `qt(1-alpha, df = n-1)` for a 1-sided lower (1- $\alpha$ )% CI
* $t^*$ = `qt(alpha, df = n-1)` for a 1-sided upper (1- $\alpha$ )% CI
* A 1-sided CI gives estimates for a lower or upper bound of the population mean.
* See Section 4.2.3 of the V&H book for more


# Confidence Intervals vs. Hypothesis Testing

* See also V&H Section 4.3.3



# What's next? 


CI's and hypothesis tests for different scenarios:

$$point~estimate \pm z^*(or~t^*)\cdot SE,~~~~~~~~test~stat = \frac{point~estimate-null~value}{SE}$$

Day | Book | Population <br> parameter | Symbol | Point estimate | Symbol | SE
--------|--------|--------|--------|--------|--------|--------
11-12 | 5.1 | Pop mean | $\mu$ | Sample mean | $\bar{x}$ | $\frac{s}{\sqrt{n}}$
13 | 5.2 | Diff in paired <br> pop means | $\mu_d$ or $\delta$ | Diff in paired <br> sample means | $\bar{x}_{d}$  | $\frac{s_d}{\sqrt{n}}$
14 | 5.3 | Diff in pop <br> means | $\mu_1-\mu_2$ | Diff in sample <br> means | $\bar{x}_1 - \bar{x}_2$  |
 | 8.1 | Pop proportion | $p$ | Sample prop | $\widehat{p}$  |
 | 8.2 | Diff in pop <br> proportions | $p_1-p_2$ | Diff in sample <br> proportions | $\widehat{p}_1-\widehat{p}_2$ |


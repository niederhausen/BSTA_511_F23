{
  "hash": "0c3f41b683973e212e6e72b2f8e236ad",
  "result": {
    "markdown": "---\ntitle: \"DRAFT: Day 14: Comparing Means with ANOVA (Section 5.5)\"\nsubtitle: \"BSTA 511/611\"\nauthor: \"Meike Niederhausen, PhD\"\ninstitute: \"OHSU-PSU School of Public Health\"\ndate: \"11/20/2023\"\ncategories: [\"Week 9\"]\nformat: \n  revealjs:\n      incremental: false\n      scrollable: true\n      chalkboard: true\n      theme: [../sky_modified_smaller_font.scss]\n      width:  1100 #1200 # 1050 #default 1050; ipad 3:4, 1600\n      height: 825 #900 #800 #default 700; 788 for 3:4, 1200\n      slide-number: true\n      html-math-method: mathjax\n  # html:\n  #   link-external-newwindow: true\n  #   toc: true\nexecute:\n  echo: true\n  freeze: auto  # re-render only when source changes\n# editor: visual\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\n\n## Topics\n\n* When to use an ANOVA\n* Different sources of variation in ANOVA\n* ANOVA assumptions\n* Post-hoc testing of differences in means\n* Running an ANOVA in R\n\n### Additional Resource\n\n* BSTA 511 textbook (Vu & Harrington)\n    * Section 5.5: Comparing means with ANOVA\n    * Section 7.9: Connection between ANOVA and regression\n\n\n\n## Disability Discrimination Example \n\n::: columns\n::: {.column width=\"50%\"}\n* The U.S. Rehabilitation Act of 1973 prohibited discrimination against people with physical disabilities. \n    * The act defined a disabled person as any individual who has a physical or mental impairment that limits the person's major life activities.\n* A 1980's study examined whether physical disabilities affect people's perceptions of employment qualifications \n    * ([Cesare, Tannenbaum, & Dalessio, 1990](https://psycnet.apa.org/record/1991-07629-001)). \n:::\n\n::: {.column width=\"50%\"}\n* Researchers prepared recorded job interviews, using _same actors and script each time_. \n* Only difference: job applicant appeared with different disabilities.\n    * _No disability_\n    * _Leg amputation_\n    * _Crutches_\n    * _Hearing impairment_\n    * _Wheelchair confinement_\n* 70 undergrad students were randomly assigned to view one of the videotapes, \n    * then __rated__ the candidate's qualifications on a __1-10 scale__.\n\n:::\n:::\n\n* The research question: __are qualifications evaluated differently depending on the applicant's presented disability?__\n\n\n\n## Load interview data from `.txt` file\n\n::: columns\n::: {.column width=\"50%\"}\n* `.txt` files are usually tab-deliminated files\n    * `.csv` files are comma-separated files\n* `read_delim` is from the `readr` package, just like `read_csv`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemploy <- read_delim(\n  file = here::here(\"data\", \"DisabilityEmployment.txt\"), \n  delim = \"\\t\",   # tab delimited\n  trim_ws = TRUE)\n```\n:::\n\n\n`trim_ws`: \tShould leading and trailing whitespace be trimmed from each field before parsing it?\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(employ)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  disability            score      \n Length:70          Min.   :1.400  \n Class :character   1st Qu.:3.700  \n Mode  :character   Median :5.050  \n                    Mean   :4.929  \n                    3rd Qu.:6.100  \n                    Max.   :8.500  \n```\n:::\n\n```{.r .cell-code}\nemploy %>% tabyl(disability)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n disability  n percent\n    amputee 14     0.2\n   crutches 14     0.2\n    hearing 14     0.2\n       none 14     0.2\n wheelchair 14     0.2\n```\n:::\n:::\n\n:::\n:::\n\n\n\n\n\n## MoRitz's tip of the day\n\nRead [OHSU's Inclusive Language Guide](https://www.ohsu.edu/sites/default/files/2021-03/OHSU%20Inclusive%20Language%20Guide_031521.pdf) (below is from pgs. 22-25)\n \n\"... an evolving tool to help OHSU members learn about and use inclusive language...\"\n\nSections on: Race and ethnicity, Immigration status, Gender and sexual orientation, and Ability (including physical, mental and chronological attributes)\n\n::: columns\n::: {.column width=\"50%\"}\n\n![](/img_slides/OHSU_Inclusive_Language_Guide_Ability_header.png){fig-align=\"center\"}\n\n![](/img_slides/OHSU_Inclusive_Language_Guide_Respectful.png){fig-align=\"center\"}\n\n![](/img_slides/OHSU_Inclusive_Language_Guide_Respectful_disability.png){fig-align=\"center\"}\n\n:::\n\n::: {.column width=\"50%\"}\n![](/img_slides/OHSU_Inclusive_Language_Guide_Avoid.png){fig-align=\"center\"}\n:::\n:::\n\n\n\n\n\n## Factor variable: Make `disability` variable a factor variable\n\n::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(employ)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 70\nColumns: 2\n$ disability <chr> \"none\", \"none\", \"none\", \"none\", \"none\", \"none\", \"none\", \"no…\n$ score      <dbl> 1.9, 2.5, 3.0, 3.6, 4.1, 4.2, 4.9, 5.1, 5.4, 5.9, 6.1, 6.7,…\n```\n:::\n\n```{.r .cell-code}\nemploy <- employ %>% \n  mutate(disability = factor(disability))\nglimpse(employ)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 70\nColumns: 2\n$ disability <fct> none, none, none, none, none, none, none, none, none, none,…\n$ score      <dbl> 1.9, 2.5, 3.0, 3.6, 4.1, 4.2, 4.9, 5.1, 5.4, 5.9, 6.1, 6.7,…\n```\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(employ)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      disability     score      \n amputee   :14   Min.   :1.400  \n crutches  :14   1st Qu.:3.700  \n hearing   :14   Median :5.050  \n none      :14   Mean   :4.929  \n wheelchair:14   3rd Qu.:6.100  \n                 Max.   :8.500  \n```\n:::\n:::\n\n\n:::\n:::\n\n\n\n## Factor variable: Change order & name of disability levels \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlevels(employ$disability)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"amputee\"    \"crutches\"   \"hearing\"    \"none\"       \"wheelchair\"\n```\n:::\n\n```{.r .cell-code}\nemploy <- employ %>% \n  mutate(\n    # make \"none\" the first level\n    # by only listing the level none, all other levels will be in original order\n    disability = fct_relevel(disability, \"none\"),\n    # change the level name amputee to amputation\n    disability = fct_recode(disability, amputation = \"amputee\")\n    )\n\nlevels(employ$disability) # note the new order and new name\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"none\"       \"amputation\" \"crutches\"   \"hearing\"    \"wheelchair\"\n```\n:::\n:::\n\n\n`fct_relevel()` and `fct_recode()` are from the `forcats` package: https://forcats.tidyverse.org/index.html. `forcats` is loaded with `library(tidyverse)`.\n\n\n\n## Data viz\n\n::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(employ, aes(x=score)) +\n  geom_density() +\n  facet_wrap(~ disability)\n```\n\n::: {.cell-output-display}\n![](Day14_bsta511_files/figure-revealjs/unnamed-chunk-7-1.png){width=480}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggridges) #<<\nggplot(employ, \n       aes(x=score,\n           y = disability,\n           fill = disability)) + \n  geom_density_ridges(alpha = 0.4) +#<<\n  theme(legend.position=\"none\")#<<\n```\n\n::: {.cell-output-display}\n![](Day14_bsta511_files/figure-revealjs/unnamed-chunk-8-1.png){width=480}\n:::\n:::\n\n:::\n:::\n\n\n\n::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(employ, aes(y=score, x = disability,\n                   fill = disability)) +\n  geom_boxplot(alpha =.3) +\n  coord_flip() +\n  geom_jitter(width =.1, alpha = 0.3) +\n  theme(legend.position = \"none\")#<<\n```\n\n::: {.cell-output-display}\n![](Day14_bsta511_files/figure-revealjs/unnamed-chunk-9-1.png){width=480}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(employ, aes(x = disability, y=score, \n    fill=disability, color=disability)) +\n  geom_dotplot(binaxis = \"y\", alpha =.5) +#<<\n  geom_hline(aes(yintercept = mean(score)), \n             lty = \"dashed\") +\n  stat_summary(fun =\"mean\", geom=\"point\", #<<\n    size = 3, color = \"grey33\", alpha =1) +#<<\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](Day14_bsta511_files/figure-revealjs/unnamed-chunk-10-1.png){width=480}\n:::\n:::\n\n:::\n:::\n\n\n\n\n\n## Hypotheses\n\nTo test for a difference in means across _k_ treatment groups:\n\n\\begin{align}\nH_0 &: \\mu_1 = \\mu_2 = ... = \\mu_k\\\\\n\\text{vs. } H_A&: \\text{At least one pair } \\mu_i \\neq \\mu_j\n\\end{align}\n\n\n__Hypothetical examples:__  \nIn which set (A or B) do you believe the evidence will be stronger \nthat at least one population differs from the others?\n\n![](/img_slides/hypothetical_disability_data_v2.png){fig-align=\"center\"}\n\n\n\n## Comparing means\n\nWhether or not two means are significantly different depends on:\n* How far apart the means are\n* How much variability there is within each treatment group\n\n__Questions:__  \n* How to measure variability between treatment groups?\n* How to measure variability within treatment groups?\n* How to compare the two measures?\n* How to determine significance?\n\n\n\n## ANOVA in base R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nempl_lm <- lm(score ~ disability, data = employ)\nanova(empl_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: score\n           Df  Sum Sq Mean Sq F value  Pr(>F)  \ndisability  4  30.521  7.6304  2.8616 0.03013 *\nResiduals  65 173.321  2.6665                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nHypotheses:\n\n\\begin{align}\nH_0 &: \\mu_{none} = \\mu_{amputation} = \\mu_{crutches} = \\mu_{hearing} =  \\mu_{wheelchair}\\\\\n\\text{vs. } H_A&: \\text{At least one pair } \\mu_i \\neq \\mu_j\n\\end{align}\n\nDo we reject or fail to reject $H_0$?\n\n\n\n## ANOVA tables\n\n::: columns\n::: {.column width=\"50%\"}\nDisability example ANOVA table\n![](/img_slides/anova_table_example_R_output.png){fig-align=\"center\"}\n\n:::\n\n::: {.column width=\"50%\"}\nGeneric ANOVA table\n![](/img_slides/anova_table.png){fig-align=\"center\"}\n:::\n:::\n\n\n\n## ANOVA table\n\n\n![](/img_slides/anova_table.png){fig-align=\"center\"}\n\n\n\n\n## ANOVA: Analysis of Variance \n\n__ANOVA__ compares the variability between treatments to the variability within treatments \n\n![](/img_slides/anova_total_variability.png){fig-align=\"center\"}\n\n\n![](/img_slides/SST_visually_unnamed-chunk-13-1.png){fig-align=\"center\" width = \"30%\"} =\n![](/img_slides/SSG_visually_unnamed-chunk-14-1.png){fig-align=\"center\" width = \"30%\"} +\n![](/img_slides/SSE_visually_unnamed-chunk-17-1.png){fig-align=\"center\" width = \"30%\"} \n\n\n\n\n## ANOVA: Analysis of Variance \n\n__Analysis of Variance (ANOVA)__ compares the variability between treatments to the variability within treatments \n\n![](/img_slides/anova_total_variability.png){fig-align=\"center\"}\n\n\n$$\\sum_{i = 1}^k \\sum_{j = 1}^{n_i}(y_{ij} -\\bar{y}_{..})^2 \\ \\ = \\ \\sum_{i = 1}^k n_i(\\bar{y}_{i.}-\\bar{y}_{..})^2 \\ \\ + \\ \\ \\sum_{i = 1}^k\\sum_{j = 1}^{n_i}(y_{ij}-\\bar{y}_{i.})^2$$\n\n\n![](/img_slides/anova_SS_total.png){fig-align=\"center\"}\n\n\n\n\n## Notation\n\n::: columns\n::: {.column width=\"40%\"}\n\n* _k_ treatment groups\n* _n_ observations in each of the _k_ treatment groups\n* Total sample size is $N=\\sum_{i=1}^{k}n_i$\n* $\\bar{y}_{i.}$ = mean of observations in treatment _i_\n* $\\bar{y}_{..}$ = mean of _all_ observations\n\n:::\n\n::: {.column width=\"60%\"}\n\n| Observation | *i* = 1      | *i* = 2      | *i* = 3      | $\\ldots$ |*i* = *k*| overall|\n| :-| ::| ::|:-:|:-:|:--:|:--:|\n| *j* = 1       | $y_{11}$ | $y_{21}$ | $y_{31}$ | $\\ldots$ | $y_{k1}$ ||\n| *j* = 2       | $y_{12}$ | $y_{22}$ | $y_{32}$ | $\\ldots$ | $y_{k2}$ ||\n| *j* = 3       | $y_{13}$ | $y_{23}$ | $y_{33}$ | $\\ldots$ | $y_{k3}$ ||\n| *j* = 4       | $y_{14}$ | $y_{24}$ | $y_{34}$ | $\\ldots$ | $y_{k4}$ ||\n| $\\vdots$    | $\\vdots$ | $\\vdots$ | $\\vdots$ | $\\ddots$ | $\\vdots$ ||\n| *j* = $n_i$     | $y_{1n}$ | $y_{2n}$ | $y_{3n}$ | $\\ldots$ | $y_{kn}$ ||\n| Means       | $\\bar{y}_{1.}$ | $\\bar{y}_{2.}$ | $\\bar{y}_{3.}$ | $\\ldots$ | $\\bar{y}_{k.}$ | $\\bar{y}_{..}$ |\n| Variance    | ${s}^2_{1.}$ | ${s}^2_{2.}$ | ${s}^2_{3.}$ | $\\ldots$ | ${s}^2_{k.}$ | ${s}^2_{..}$ |\n\n:::\n:::\n\n\n\n\n## Total Sums of Squares Visually\n\n::: columns\n::: {.column width=\"40%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](Day14_bsta511_files/figure-revealjs/unnamed-chunk-12-1.png){width=480}\n:::\n:::\n\n:::\n::: {.column width=\"60%\"}\nTotal Sums of Squares:\n\n$$\\sum_{i = 1}^k \\sum_{j = 1}^{n_i}(y_{ij} -\\bar{y}_{..})^2 = (N-1)s^2_{..}$$\nwhere $N=\\sum_{i=1}^{k}n_i$ is the total sample size and $s^2_{..}$ is the grand standard deviation of all the observations\n\n* This is the sum of the squared differences between each observed $y_{ij}$ value and the *grand mean*, $\\bar{y}_{..}$. \n\n* That is, it is the total deviation of the $y_{ij}$'s from the grand mean. \n\n:::\n:::\n\n\n\n## Calculate Total Sums of Squares \n\n::: columns\n::: {.column width=\"40%\"}\nTotal Sums of Squares:\n\n$$\\sum_{i = 1}^k \\sum_{j = 1}^{n_i}(y_{ij} -\\bar{y}_{..})^2 = (N-1)s^2_{..}$$\nwhere $N=\\sum_{i=1}^{k}n_i$ is the total sample size and $s^2_{..}$ is the grand standard deviation of all the observations\n:::\n\n::: {.column width=\"60%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(Ns <- employ %>% group_by(disability) %>% count())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 2\n# Groups:   disability [5]\n  disability     n\n  <fct>      <int>\n1 none          14\n2 amputation    14\n3 crutches      14\n4 hearing       14\n5 wheelchair    14\n```\n:::\n\n```{.r .cell-code}\n(SST <- (sum(Ns$n) - 1) * sd(employ$score)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 203.8429\n```\n:::\n:::\n\n\n:::\n:::\n\n\n## ANOVA: Analysis of Variance \n\n__ANOVA__ compares the variability between treatments to the variability within treatments \n\n![](/img_slides/anova_total_variability.png){fig-align=\"center\"}\n\n\n<img src=\"img/SST_visually_unnamed-chunk-13-1.png){fig-align=\"center\" width = \"30%\"} =\n<img src=\"img/SSG_visually_unnamed-chunk-14-1.png){fig-align=\"center\" width = \"30%\"} +\n<img src=\"img/SSE_visually_unnamed-chunk-17-1.png){fig-align=\"center\" width = \"30%\"} \n\n\n\n\n\n## Sums of Squares due to Treatments Visually (\"between\" treatments)\n\n::: columns\n::: {.column width=\"40%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](Day14_bsta511_files/figure-revealjs/unnamed-chunk-14-1.png){width=480}\n:::\n:::\n\n:::\n::: {.column width=\"60%\"}\nSums of Squares due to Treatments:\n\n$$SSTr = \\sum_{i = 1}^k n_i(\\bar{y}_{i.}-\\bar{y}_{..})^2$$\n\n* This is the sum of the squared differences between each *treatment* mean, $\\bar{y}_{i.}$, and the *grand mean*, $\\bar{y}_{..}$. \n\n* That is, it is the deviation of the treatment means from the grand mean.\n\n* Also called the Model SS, or $SS_{model}.$\n\n:::\n:::\n\n\n\n\n## Calculate Sums of Squares due to Treatments (\"between\" treatments)\n\n::: columns\n::: {.column width=\"40%\"}\nSums of Squares due to Treatments:\n\n$$SSTr = \\sum_{i = 1}^k n_i(\\bar{y}_{i.}-\\bar{y}_{..})^2$$\n\n![](/img_slides/SSG_visually_unnamed-chunk-14-1.png){fig-align=\"center\"}\n\n:::\n\n::: {.column width=\"60%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nxbar_groups <- employ %>% \n  group_by(disability) %>% \n  summarise(mean = mean(score))\nxbar_groups\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 2\n  disability  mean\n  <fct>      <dbl>\n1 none        4.9 \n2 amputation  4.43\n3 crutches    5.92\n4 hearing     4.05\n5 wheelchair  5.34\n```\n:::\n\n```{.r .cell-code}\n(SSTr <- 14*sum(\n  (xbar_groups$mean - mean(employ$score))^2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 30.52143\n```\n:::\n:::\n\n\n:::\n:::\n\n\n## ANOVA: Analysis of Variance \n\n__ANOVA__ compares the variability between treatments to the variability within treatments \n\n![](/img_slides/anova_total_variability.png){fig-align=\"center\"}\n\n\n![](/img_slides/SST_visually_unnamed-chunk-13-1.png){fig-align=\"center\" width = \"30%\"} =\n![](/img_slides/SSG_visually_unnamed-chunk-14-1.png){fig-align=\"center\" width = \"30%\"} +\n![](/img_slides/SSE_visually_unnamed-chunk-17-1.png){fig-align=\"center\" width = \"30%\"} \n\n\n\n\n## Sums of Squares Error Visually (within treatments)\n\n::: columns\n::: {.column width=\"40%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](Day14_bsta511_files/figure-revealjs/unnamed-chunk-16-1.png){width=480}\n:::\n:::\n\n:::\n::: {.column width=\"60%\"}\nSums of Squares Error:\n\n$$SSE = \\sum_{i = 1}^k\\sum_{j = 1}^{n_i}(y_{ij}-\\bar{y}_{i.})^2 = \\sum_{i = 1}^k(n_i-1)s_{i.}^2$$\nwhere $s_{i.}$ is the standard deviation of the $i^{th}$ treatment\n\n* This is the sum of the squared differences between each observed $y_{ij}$ value and its treatment mean $\\bar{y}_{i.}$. \n\n* That is, it is the deviation of the $y_{ij}$'s from the predicted score by treatment.\n\n* Also called the residual sums of squares, or $SS_{residual}.$\n\n:::\n:::\n\n\n\n\n\n## Calculate Sums of Squares Error (within treatments)\n\n::: columns\n::: {.column width=\"60%\"}\nSums of Squares Error:\n\n$$SSE = \\sum_{i = 1}^k\\sum_{j = 1}^{n_i}(y_{ij}-\\bar{y}_{i.})^2 = \\sum_{i = 1}^k(n_i-1)s_{i.}^2$$\nwhere $s_{i.}$ is the standard deviation of the $i^{th}$ treatment\n\n<img src=\"img/SSE_visually_unnamed-chunk-17-1.png\" width=\"40%\" height=\"100%\"> \n\n:::\n\n::: {.column width=\"40%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsd_groups <- employ %>% \n  group_by(disability) %>% \n  summarise(SD = sd(score))\nsd_groups\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 2\n  disability    SD\n  <fct>      <dbl>\n1 none        1.79\n2 amputation  1.59\n3 crutches    1.48\n4 hearing     1.53\n5 wheelchair  1.75\n```\n:::\n\n```{.r .cell-code}\n(SSE <- sum(\n  (14-1)*sd_groups$SD^2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 173.3214\n```\n:::\n:::\n\n\n:::\n:::\n\n\n\n## Verify _SST = SSTr + SSE_\n\n__ANOVA__ compares the variability between treatments to the variability within treatments \n\n![](/img_slides/anova_total_variability.png){fig-align=\"center\"}\n\n$$\\sum_{i = 1}^k \\sum_{j = 1}^{n_i}(y_{ij} -\\bar{y}_{..})^2 \\ \\ = \\ \\ n_i\\sum_{i = 1}^k(\\bar{y}_{i.}-\\bar{y}_{..})^2 \\ \\ + \\ \\ \\sum_{i = 1}^k\\sum_{j = 1}^{n_i}(y_{ij}-\\bar{y}_{i.})^2$$\n\n$$(N-1)s^2_{..} \\ \\ = \\ \\sum_{i = 1}^k n_i(\\bar{y}_{i.}-\\bar{y}_{..})^2 \\ \\ + \\ \\ \\sum_{i = 1}^k(n_i-1)s_{i.}^2$$\n\n![](/img_slides/anova_SS_total.png){fig-align=\"center\" width = \"30%\"}\n\n::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nSST\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 203.8429\n```\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nSSTr + SSE\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 203.8429\n```\n:::\n:::\n\n:::\n:::\n\n\n## ANOVA table\n\n<img src=\"img/SS_all3_vertical.png\" width=\"14%\" height=\"100%\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n<img align=top src=\"img/anova_table.png\" width=\"65%\" height=\"100%\">\n\n\n\n## Thinking about the F-statistic\n\n::: columns\n::: {.column width=\"50%\"}\n__[If the treatments are actually different, then which of these is more accurate?]{style=\"color:green\"}\n\n1. The variability between treatments should be higher than the variability within treatments\n1. The variability within treatments should be higher than the variability between treatments\n\n![](/img_slides/hypothetical_disability_data_v2.png){fig-align=\"center\"}\n:::\n\n::: {.column width=\"50%\"}\n__[If there really is a difference between the treatments, we would expect the F-statistic to be which of these: ]{style=\"color:green\"}\n\n1. Higher than we would observe by random chance\n1. Lower than we would observe by random chance\n\n$$F = \\frac{MSG}{MSE}$$\n\n:::\n:::\n\n\n\n\n\n## ANOVA in base R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nempl_lm <- lm(score ~ disability, data = employ)\ntidy(anova(empl_lm))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 6\n  term          df sumsq meansq statistic p.value\n  <chr>      <int> <dbl>  <dbl>     <dbl>   <dbl>\n1 disability     4  30.5   7.63      2.86  0.0301\n2 Residuals     65 173.    2.67     NA    NA     \n```\n:::\n:::\n\n\nHypotheses:\n\n\\begin{align}\nH_0 &: \\mu_{none} = \\mu_{amputation} = \\mu_{crutches} = \\mu_{hearing} =  \\mu_{wheelchair}\\\\\n\\text{vs. } H_A&: \\text{At least one pair } \\mu_i \\neq \\mu_j\n\\end{align}\n\nDo we reject or fail to reject $H_0$?\n\n\n\n## Conclusion to hypothesis test\n\n\\begin{align}\nH_0 &: \\mu_{none} = \\mu_{amputation} = \\mu_{crutches} = \\mu_{hearing} =  \\mu_{wheelchair}\\\\\n\\text{vs. } H_A&: \\text{At least one pair } \\mu_i \\neq \\mu_j\n\\end{align}\n\n::: columns\n::: {.column width=\"60%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(anova(empl_lm))  # cleaner anova output\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 6\n  term          df sumsq meansq statistic p.value\n  <chr>      <int> <dbl>  <dbl>     <dbl>   <dbl>\n1 disability     4  30.5   7.63      2.86  0.0301\n2 Residuals     65 173.    2.67     NA    NA     \n```\n:::\n\n```{.r .cell-code}\nround(broom::tidy(anova(empl_lm))$p.value[1],2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.03\n```\n:::\n:::\n\n:::\n\n::: {.column width=\"40%\"}\n* Use $\\alpha$ = 0.05.\n* Do we reject or fail to reject $H_0$?\n\n__Conclusion statement__:\n* There is sufficient evidence that at least one of the disability groups has a mean employment score statistically different from the other groups. ( $p$-value = 0.03).\n\n:::\n:::\n\n\n## Assumptions for ANOVA \n\n::: columns\n::: {.column width=\"50%\"}\n[__IF__ the following conditions hold:]{style=\"color:green\"}\n\n1. the null hypothesis is true\n1. sample sizes in each treatment group are large (each $n \\ge 30$) \n    * OR the data are relatively normally distributed\n1. variability is \"similar\" in all treatment groups:\n    * Is the within treatment group variability about the same for each treatment group?\n    * As a rough rule of thumb, this assumption is _violated if the standard deviation of one treatment group is more than double the standard deviation of another treatment group_\n\n[__THEN__ the sampling distribution of the   \n__F-statistic__ is an __F-distribution__]{style=\"color:green\"}\n\n:::\n\n::: {.column width=\"50%\"}\nChecking the __equal variance__ assumption:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsd_groups # previously defined\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 2\n  disability    SD\n  <fct>      <dbl>\n1 none        1.79\n2 amputation  1.59\n3 crutches    1.48\n4 hearing     1.53\n5 wheelchair  1.75\n```\n:::\n\n```{.r .cell-code}\nmax(sd_groups$SD) / min(sd_groups$SD)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.210425\n```\n:::\n:::\n\n\n:::\n:::\n\n\n\n## Testing variances (Assumption 3)\n\n__Bartlett’s test for equal variances__\n\n* $H_0:$ variances of treatment levels are equal\n* $H_A:$ variances of treatment levels are NOT equal\n\n_Note: $H_A$ is same as saying that at least one of the treatment levels has a different variance__\n\n* Caution: Bartlett's test assumes the data in each treatment group are normally distributed. Do not use if data do not satisfy the normality assumption. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nbartlett.test(score ~ disability, data = employ)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tBartlett test of homogeneity of variances\n\ndata:  score by disability\nBartlett's K-squared = 0.7016, df = 4, p-value = 0.9511\n```\n:::\n:::\n\n\n* Levene's test for equality of variances is not as restrictive: see https://www.statology.org/levenes-test-r/ \n\n\n## The F-distribution\n\n::: columns\n::: {.column width=\"50%\"}\nThe F-distribution is skewed right:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Day14_bsta511_files/figure-revealjs/unnamed-chunk-24-1.png){width=480}\n:::\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\"}\nThe __F-distribution__ has two degrees of freedom:\n\n* one for the numerator of the ratio (k – 1) and \n* one for the denominator (N – k)\n\n$p$-__value__:  \nFor F-statistics, the _p_-value (the area as extreme or more extreme) is always the __upper tail__.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# p-value using F-distribution\npf(2.8646, df1=5-1, df2=70-5, \n   lower.tail = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.02999488\n```\n:::\n:::\n\n\n:::\n:::\n\n\n\n## Which treatment groups are statistically different?\n\n::: columns\n::: {.column width=\"40%\"}\n* So far we've only determined that at least one of the treatment groups is different from the others, but we don't know which.\n\n* What's your guess?\n\n:::\n\n::: {.column width=\"60%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](Day14_bsta511_files/figure-revealjs/unnamed-chunk-26-1.png){width=480}\n:::\n:::\n\n\n:::\n:::\n\n\n# Post-hoc testing for ANOVA\n\n_determining which groups are statistically different_\n\n\n## Post-hoc testing: pairwise t-tests\n\n::: {.column width=\"40%\"}\n* In post-hoc testing we run all the pairwise t-tests comparing the means from each pair of groups.\n* With 5 groups, this involves doing ${5 \\choose 2} = \\frac{5!}{2!3!} = \\frac{5\\cdot 4}{2}= 10$ different pairwise tests.\n\n__Problem:__\n\nAlthough test has an $\\alpha$ chance of a Type I error (finding a difference between a pair that aren't different), the overall Type I error rate will be much higher when running many tests simultaneously.\n\n:::\n\n::: {.column width=\"60%\"}\n\\begin{align}\nP(\\text{making an error}) = & \\alpha\\\\\nP(\\text{not making an error}) = & 1-\\alpha\\\\\nP(\\text{not making an error in m tests}) = & (1-\\alpha)^m\\\\\nP(\\text{making at least 1 error in m tests}) = & 1-(1-\\alpha)^m\n\\end{align}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Day14_bsta511_files/figure-revealjs/unnamed-chunk-27-1.png){width=960}\n:::\n:::\n\n\n:::\n:::\n\n\n\n## The Bonferroni Correction (1/2)\n\n::: columns\n::: {.column width=\"50%\"}\nA very conservative (but very popular) approach is to divide the $\\alpha$ level by how many tests $m$ are being done:\n\n$$\\alpha_{Bonf} = \\frac{\\alpha}{m}$$\n\n* This is equivalent to multiplying the  \n_p_-values by m:\n\n$$p\\textrm{-value} < \\alpha_{Bonf} = \\frac{\\alpha}{m}$$ \nis the same as\n$$m \\cdot (p\\textrm{-value}) < \\alpha$$\nThe Bonferroni correction is popular since it's very easy to implement.\n:::\n\n::: {.column width=\"50%\"}\n* The __plot below__ shows the __[likelihood of making at least one Type I error]{style=\"color:green\"} depending on how may tests are done.\n* Notice the likelihood decreases very quickly\n    * Unfortunately the likelihood of a Type II error is increasing as well\n    * It becomes \"harder\" and harder to reject $H_0$ if doing many tests.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Day14_bsta511_files/figure-revealjs/unnamed-chunk-28-1.png){width=960}\n:::\n:::\n\n\n:::\n:::\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n## The Bonferroni Correction (2/2)\n\n::: columns\n::: {.column width=\"50%\"}\nPairwise t-tests without any _p_-value adjustments:\n\n::: {.cell}\n\n```{.r .cell-code}\npairwise.t.test(employ$score, \n                employ$disability, \n                p.adj=\"none\") \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPairwise comparisons using t tests with pooled SD \n\ndata:  employ$score and employ$disability \n\n           none   amputation crutches hearing\namputation 0.4477 -          -        -      \ncrutches   0.1028 0.0184     -        -      \nhearing    0.1732 0.5418     0.0035   -      \nwheelchair 0.4756 0.1433     0.3520   0.0401 \n\nP value adjustment method: none \n```\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\nPairwise t-tests __[with Bonferroni _p_-value adjustments]{style=\"color:green\"}:\n\n::: {.cell}\n\n```{.r .cell-code}\npairwise.t.test(employ$score,  \n                employ$disability, \n                p.adj=\"bonferroni\")  #<<\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPairwise comparisons using t tests with pooled SD \n\ndata:  employ$score and employ$disability \n\n           none  amputation crutches hearing\namputation 1.000 -          -        -      \ncrutches   1.000 0.184      -        -      \nhearing    1.000 1.000      0.035    -      \nwheelchair 1.000 1.000      1.000    0.401  \n\nP value adjustment method: bonferroni \n```\n:::\n:::\n\n\nSince there were 10 tests, all the _p_-values were multiplied by 10.\n:::\n:::\n\n\n\n\n\n## Tukey's Honest Significance Test (HSD) (1/3)\n\n::: columns\n::: {.column width=\"40%\"}\n* Tukey's Honest Significance Test (HSD) controls the \"family-wise probability\" of making a Type I error using a much less conservative method than Bonferroni\n* __It is specific to ANOVA__\n* In addition to adjusted _p_-values, it also calculates Tukey adjusted CI's \n\nThe function `TukeyHSD()` creates a set of __confidence intervals__ on the differences between means with the specified __family-wise probability of coverage__.\n:::\n\n::: {.column width=\"60%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\n# need to run the model as an `aov` instead of `lm`\nempl_aov <- aov(score ~ disability, data = employ) #<<\nanova(empl_aov)  #<<\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: score\n           Df  Sum Sq Mean Sq F value  Pr(>F)  \ndisability  4  30.521  7.6304  2.8616 0.03013 *\nResiduals  65 173.321  2.6665                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n:::\n:::\n\n\n\n\n## Tukey's Honest Significance Test (HSD) (2/3)\n\nBoth Tukey HSD p-values and CI's for all pairwise differences.\n\n::: {.cell}\n\n```{.r .cell-code}\nTukeyHSD(x=empl_aov, conf.level = 0.95) #<<\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = score ~ disability, data = employ)\n\n$disability\n                            diff        lwr        upr     p adj\namputation-none       -0.4714286 -2.2031613  1.2603042 0.9399911\ncrutches-none          1.0214286 -0.7103042  2.7531613 0.4686233\nhearing-none          -0.8500000 -2.5817328  0.8817328 0.6442517\nwheelchair-none        0.4428571 -1.2888756  2.1745899 0.9517374\ncrutches-amputation    1.4928571 -0.2388756  3.2245899 0.1232819\nhearing-amputation    -0.3785714 -2.1103042  1.3531613 0.9724743\nwheelchair-amputation  0.9142857 -0.8174470  2.6460185 0.5781165\nhearing-crutches      -1.8714286 -3.6031613 -0.1396958 0.0277842\nwheelchair-crutches   -0.5785714 -2.3103042  1.1531613 0.8812293\nwheelchair-hearing     1.2928571 -0.4388756  3.0245899 0.2348141\n```\n:::\n:::\n\n\n\n\n\n## Tukey's Honest Significance Test (HSD) (3/3)\n\n::: columns\n::: {.column width=\"60%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(TukeyHSD(x=empl_aov, conf.level = 0.95))\n```\n\n::: {.cell-output-display}\n![](Day14_bsta511_files/figure-revealjs/unnamed-chunk-34-1.png){width=960}\n:::\n:::\n\n:::\n\n::: {.column width=\"40%\"}\n* Visualization of pairwise CI's\n\n* __[Which pair(s) of disabilities are significant after Tukey's adjustments?]{style=\"color:green\"}\n\n:::\n:::\n\n\n\n## There are many more multiple testing adjustment procedures\n\n::: columns\n::: {.column width=\"50%\"}\n* Bonferroni is popular because it's so easy to apply\n* Tukey's HSD is usually used for ANOVA\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# default is Holm's adjustments\npairwise.t.test(employ$score, \n                employ$disability) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPairwise comparisons using t tests with pooled SD \n\ndata:  employ$score and employ$disability \n\n           none  amputation crutches hearing\namputation 1.000 -          -        -      \ncrutches   0.719 0.165      -        -      \nhearing    0.866 1.000      0.035    -      \nwheelchair 1.000 0.860      1.000    0.321  \n\nP value adjustment method: holm \n```\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\nPairwise t-tests with __false discovery rate (fdr)__ _p_-value adjustments (popular in omics):\n\n::: {.cell}\n\n```{.r .cell-code}\npairwise.t.test(employ$score, \n                employ$disability, \n                p.adj=\"fdr\") #<<\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPairwise comparisons using t tests with pooled SD \n\ndata:  employ$score and employ$disability \n\n           none  amputation crutches hearing\namputation 0.528 -          -        -      \ncrutches   0.257 0.092      -        -      \nhearing    0.289 0.542      0.035    -      \nwheelchair 0.528 0.287      0.503    0.134  \n\nP value adjustment method: fdr \n```\n:::\n:::\n\n\n:::\n:::\n\n\n# Multiple testing\n\n_post-hoc testing vs. testing many outcomes_\n\n\n![](/img_slides/xkcd_significant2.png){fig-align=\"center\"}\n\nhttps://xkcd.com/882/\n\n\n\n## Multiple testing: controlling the Type I error rate\n\n::: columns\n::: {.column width=\"40%\"}\n* The multiple testing issue is not unique to ANOVA post-hoc testing.\n* It is also a concern when running separate tests for many related outcomes.\n* __[Beware of _p_-hacking!]{style=\"color:darkorange\"}\n\n__Problem:__\n\nAlthough test has an $\\alpha$ chance of a Type I error (finding a difference between a pair that aren't different), the overall Type I error rate will be much higher when running many tests simultaneously.\n\n:::\n\n::: {.column width=\"60%\"}\n\\begin{align}\nP(\\text{making an error}) = & \\alpha\\\\\nP(\\text{not making an error}) = & 1-\\alpha\\\\\nP(\\text{not making an error in m tests}) = & (1-\\alpha)^m\\\\\nP(\\text{making at least 1 error in m tests}) = & 1-(1-\\alpha)^m\n\\end{align}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Day14_bsta511_files/figure-revealjs/unnamed-chunk-37-1.png){width=960}\n:::\n:::\n\n\n:::\n:::\n\n\n\n\n\n",
    "supporting": [
      "Day14_bsta511_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
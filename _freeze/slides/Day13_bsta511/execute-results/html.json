{
  "hash": "40411ef67fbb727ad7fcbd8421f01ed0",
  "result": {
    "markdown": "---\ntitle: \"DRAFT - Day 17: Chi-squared tests (Sections 8.3-8.4)\"\nsubtitle: \"BSTA 511/611\"\nauthor: \"Meike Niederhausen, PhD\"\ninstitute: \"OHSU-PSU School of Public Health\"\ndate: \"11/13/2023\"\ncategories: [\"Week 8\"]\nformat: \n  revealjs:\n      incremental: false\n      scrollable: true\n      chalkboard: true\n      theme: [../sky_modified_smaller_font.scss]\n      width:  1100 #1200 # 1050 #default 1050; ipad 3:4, 1600\n      height: 825 #900 #800 #default 700; 788 for 3:4, 1200\n      slide-number: true\n      html-math-method: mathjax\n  # html:\n  #   link-external-newwindow: true\n  #   toc: true\nexecute:\n  echo: true\n  freeze: auto  # re-render only when source changes\n# editor: visual\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\n\n\n## MoRitz's tip of the day\n\n__Add text to a plot__ using `annotate()`:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(NULL, aes(c(0,4))) +  # no dataset, create axes for x from 0 to 4\n  geom_area(stat = \"function\", fun = dchisq, args = list(df=2), \n            fill = \"blue\", xlim = c(0, 1.0414)) +\n  geom_area(stat = \"function\", fun = dchisq, args = list(df=2),\n            fill = \"violet\", xlim = c(1.0414, 4)) +\n  geom_vline(xintercept = 1.0414) +  # vertical line at x = 1.0414\n  annotate(\"text\", x = 1.1, y = .4, # add text at specified (x,y) coordinate\n           label = \"chi-squared = 1.0414\", hjust=0, size=6) + \n  annotate(\"text\", x = 1.3, y = .1, \n           label = \"p-value = 0.59\", hjust=0, size=6) \n```\n\n::: {.cell-output-display}\n![](Day13_bsta511_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n\n## Where are we?\n\n<br>\n<br>\n\n![](/img_slides/flowchart_511_continuous_categorical.png){fig-align=\"center\"}\n\n## Where are we? Categorical outcome zoomed in\n\n<br>\n<br>\n\n![](/img_slides/flowchart_only_categorical.png){fig-align=\"center\"}\n\n\n\n## Goals for today (Sections 8.3-8.4)\n\n\n* Statistical inference for __categorical data__ when either are \n    * comparing __more than two groups__, \n    * or have categorical outcomes that have __more than 2 levels__, \n    * or both\n\n* Chi-squared tests of association (independence)\n    * Hypotheses\n    * test statistic\n    * Chi-squared distribution\n    * p-value\n    * technical conditions (assumptions)\n    * conclusion\n    * R: `chisq.test()`\n\n* Fisher's Exact Test\n* Chi-squared test vs. testing difference in proportions\n    * Test of Homogeneity\n\n\n\n# Chi-squared tests of association (independence)\n\nTesting the association (independence) between two categorical variables\n\n\n\n## Is there an association between depression and being physically active?\n\n* Data sampled from the NHANES R package:\n    * American National Health and Nutrition Examination Surveys\n    * Collected 2009-2012 by US National Center for Health Statistics (NCHS) \n    * `NHANES` dataset: 10,000 rows, resampled from `NHANESraw` to undo oversampling effects \n        * Treat it as a simple random sample from the US population (for pedagogical purposes)\n\n\n* __`Depressed`__\n    * Self-reported _number of days where participant felt down, depressed or hopeless_. \n    * One of None, Several, or Most (more than half the days).\n    * Reported for participants aged 18 years or older. \n\n* __`PhysActive`__\n    * _Participant does moderate or vigorous-intensity sports, fitness or recreational activities_ (Yes or No). \n    * Reported for participants 12 years or older.\n\n\n\n\n\n\n\n## Hypotheses for a Chi-squared test of association (independence)\n\n::: columns\n::: {.column width=\"48.5%\"}\n__Generic wording:__\n\nTest of \"__association__\" wording\n\n- $H_0$:  There is no association between the two variables  \n\n- $H_A$:  There is an association between the two variables \n\nTest of \"__independence__\" wording\n\n- $H_0$:  The variables are independent\n\n- $H_A$:  The variables are not independent\n\n:::\n::: {.column width=\"3%\"}\n:::\n::: {.column width=\"48.5%\"}\n\n__For our example:__\n\nTest of \"__association__\" wording\n\n- $H_0$:  There is no association between depression and physical activity\n\n- $H_A$: There is an association between depression and physical activity\n\nTest of \"__independence__\" wording\n\n- $H_0$:  The variables depression and physical activity are independent\n\n- $H_A$: The variables depression and physical activity are not independent\n\n:::\n:::\n\n::: {.callout-warning icon=false} \n## No symbols \nFor chi-squared test hypotheses we do not have versions using \"symbols\" like we do with tests of means or proportions. \n:::\n\n\n## Data from NHANES {.nostretch}\n\n* Results below are from \n    * a random sample of 400 adults (≥ 18 yrs old) \n    * with data for both the depression `Depressed` and physically active (`PhysActive`) variables.\n\n![](/img_slides/ChiSq_Table_Obs_DepressionPA.png){fig-align=\"center\" width=70%}\n\n* What does it mean for the variables to be independent?\n\n\n\n## $H_0$: Variables are Independent\n\n::: columns\n::: {.column width=\"48.5%\"}\n* Recall from Chapter 2, that events $A$ and $B$ are independent if and only if\n\n$$P(A~and~B)=P(A)P(B)$$\n\n* If depression and being physically active are independent variables, then _theoretically_ this condition needs to hold for _every combination of levels_, i.e.\n\n::: {style=\"font-size: 90%;\"}\n\n\\begin{align}\nP(None~and~Yes) &= P(None)P(Yes)\\\\\nP(None~and~No) &= P(None)P(No)\\\\\nP(Several~and~Yes) &= P(Several)P(Yes)\\\\\nP(Several~and~No) &= P(Several)P(No)\\\\\nP(Most~and~Yes) &= P(Most)P(Yes)\\\\\nP(Most~and~No) &= P(Most)P(No)\n\\end{align}\n\n:::\n\n:::\n::: {.column width=\"2%\"}\n:::\n::: {.column width=\"38.5%\"}\n![](/img_slides/ChiSq_Table_Obs_DepressionPA.png){fig-align=\"center\"}\n\n::: {style=\"font-size: 90%;\"}\n\n\\begin{align}\nP(None~and~Yes) &= \\frac{314}{400}\\cdot\\frac{226}{400}\\\\\n & ...\\\\\nP(Most~and~No) &= \\frac{28}{400}\\cdot\\frac{174}{400}\n\\end{align}\n\n:::\n\nWith these probabilities, for each cell of the table\nwe calculate the __expected__ counts for each cell under the $H_0$ hypothesis that the variables are independent\n:::\n:::\n\n\n## Expected counts (if variables are independent)\n\n::: columns\n::: {.column width=\"60%\"}\n+ The expected counts (if $H_0$ is true & the variables are independent) for each cell are\n    + $np$ = total table size $\\cdot$ probability of cell\n\nExpected count of Yes & None:\n\n\\begin{align}\n400 \\cdot & P(None~and~Yes)\\\\\n &= 400 \\cdot P(None)P(Yes)\\\\\n &= 400 \\cdot\\frac{314}{400}\\cdot\\frac{226}{400}\\\\\n &= \\frac{314\\cdot 226}{400} \\\\\n &=  177.41\\\\\n &= \\frac{\\text{column total}\\cdot \\text{row total}}{\\text{table total}}\n\\end{align}\n\n:::\n\n::: {.column width=\"40%\"}\n![](/img_slides/ChiSq_Table_Obs_DepressionPA.png){fig-align=\"center\"}\n\n* If depression and being physically active are __independent__ variables \n  * (as assumed by $H_0$), \n* then the __observed counts should be close to the expected counts__ for each cell of the table\n\n:::\n:::\n\n\n## Observed vs. Expected counts\n\n::: columns\n::: {.column width=\"40%\"}\n* The __observed__ counts are the counts in the 2-way table summarizing the data\n\n![](/img_slides/ChiSq_Table_Obs_DepressionPA.png){fig-align=\"center\"}\n\n<br>\n\nExpected count for cell $i,j$ :\n:::\n\n::: {.column width=\"60%\"}\n* The __expected__ counts are the counts the we would expect to see in the 2-way table if there was no association between depression and being physically activity\n\n![](/img_slides/ChiSq_Table_Expected_DepressionPA.png){fig-align=\"center\"}\n\n:::\n:::\n\n$$\\textrm{Expected Count}_{\\textrm{row } i,\\textrm{ col }j}=\\frac{(\\textrm{row}~i~ \\textrm{total})\\cdot(\\textrm{column}~j~ \\textrm{total})}{\\textrm{table total}}$$\n\n\n## The $\\chi^2$ test statistic\n\n::: columns\n::: {.column width=\"45%\"}\nTest statistic for a test of association (independence):\n\n$$\\chi^2 = \\sum_{\\textrm{all cells}} \\frac{(\\textrm{observed} - \\text{expected})^2}{\\text{expected}}$$\n\n* When the variables are independent, the observed and expected counts should be close to each other\n\n:::\n\n::: {.column width=\"55%\"}\n![](/img_slides/ChiSq_Table_Expected_brief_DepressionPA.png){fig-align=\"center\"}\n:::\n:::\n\n<hr>\n\n::: {style=\"font-size: 90%;\"}\n\n\\begin{align}\n\\chi^2 &=  \\sum\\frac{(O-E)^2}{E} \\\\\n&= \\frac{(199-177.41)^2}{177.41} + \\frac{(26-32.77)^2}{32.77} + \\ldots + \\frac{(27-12.18)^2}{12.18} \\\\\n&=  41.2\n\\end{align}\n\n:::\n\nIs this value big? Big enough to reject $H_0$?\n\n\n## The $\\chi^2$ distribution & calculating the _p_-value\n\n::: columns\n::: {.column width=\"60%\"}\nThe $\\chi^2$ distribution shape depends on its degrees of freedom\n\n* It's skewed right for smaller df,\n    * gets more symmetric for larger df\n* __[df = (# rows-1) x (# columns-1)]{style=\"color:green\"}__\n\n![](/img_slides/chisq_density.png){fig-align=\"center\"}\n:::\n\n::: {.column width=\"40%\"}\n* The __[p-value]{style=\"color:darkorange\"}__ is always the __[area to the right]{style=\"color:darkorange\"}__ of the test statistic for a $\\chi^2$ test.\n* We can use the `pchisq` function in R to calculate the probability of being at least as big as the $\\chi^2$ test statistic:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npv <- pchisq(41.2, df = 2, \n       lower.tail = FALSE)\npv\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.131185e-09\n```\n:::\n:::\n\n\n\nWhat's the conclusion to the $\\chi^2$ test?\n:::\n:::\n\n\n## Conclusion {.nostretch}\n\nRecall the hypotheses to our $\\chi^2$ test:\n\n- $H_0$:  There is __no association__ between depression and being physically activity\n\n- $H_A$: There is __an association__ between depression and being physically activity\n\n::: columns\n::: {.column width=\"50%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Day13_bsta511_files/figure-html/unnamed-chunk-4-1.png){width=768}\n:::\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\"}\n__Conclusion:__\n\nBased a random sample of 400 US adults from 2009-2012, there is sufficient evidence that there is an association between depression and being physically activity (_p_-value < 0.001).\n:::\n:::\n\n::: {.callout-warning} \nIf we fail to reject, we DO NOT have evidence of no association. \n:::\n\n\n## Technical conditions\n\n* __Independence__\n    * Each case (person) that contributes a count to the table must be independent of all the other cases in the table\n        * In particular, observational units cannot be represented in more than one cell.\n        * For example, someone cannot choose both \"Several\" and \"Most\" for depression status. They have to choose exactly one option for each variable.\n\n<hr>\n::: columns\n::: {.column width=\"65%\"}\n* __Sample size__\n    * In order for the distribution of the test statistic to be appropriately modeled by a chi-squared distribution we need\n    * __[ 2 $\\times$ 2 table:]{style=\"color:darkorange\"}__\n        * __expected counts are at least 10 for each cell__\n    * __[larger tables:   ]{style=\"color:darkorange\"}__\n        * __no more than 1/5 of the expected counts are less than 5__, and \n        * __all expected counts are greater than 1__\n:::\n\n::: {.column width=\"35%\"}\n![](/img_slides/ChiSq_Table_Expected_brief_DepressionPA.png){fig-align=\"center\"}\n:::\n:::\n\n\n# Chi-squared tests in R\n\n\n## Depression vs. physical activity dataset\n\n::: columns\n::: {.column width=\"50%\"}\n\n Create dataset based on results table:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nDepPA <- tibble(\n  Depression = c(rep(\"None\", 314), \n         rep(\"Several\", 58),\n         rep(\"Most\", 28)),\n  PA = c(rep(\"Yes\", 199),  # None\n          rep(\"No\", 115),\n          rep(\"Yes\", 26), # Several\n          rep(\"No\", 32),\n          rep(\"Yes\", 1), # Most\n          rep(\"No\", 27))\n)\n```\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\"}\n ![](/img_slides/ChiSq_Table_Expected_brief_DepressionPA.png){fig-align=\"center\"}\n \nSummary table of data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nDepPA %>% tabyl(Depression, PA) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n Depression  No Yes\n       Most  27   1\n       None 115 199\n    Several  32  26\n```\n:::\n\n```{.r .cell-code}\n# base R:\ntable(DepPA)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          PA\nDepression  No Yes\n   Most     27   1\n   None    115 199\n   Several  32  26\n```\n:::\n:::\n\n\n:::\n:::\n\n\n## $\\chi^2$ test in R using dataset\n\nIf only have 2 columns in the dataset:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(ChisqTest_DepPA <- chisq.test(table(DepPA)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPearson's Chi-squared test\n\ndata:  table(DepPA)\nX-squared = 41.171, df = 2, p-value = 1.148e-09\n```\n:::\n:::\n\n\n\nIf have >2 columns in the dataset, we need to specify which columns to table:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(ChisqTest_DepPA <- chisq.test(table(DepPA$Depression, DepPA$PA)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPearson's Chi-squared test\n\ndata:  table(DepPA$Depression, DepPA$PA)\nX-squared = 41.171, df = 2, p-value = 1.148e-09\n```\n:::\n:::\n\n\n\n`tidy()` the output (from `broom` package):\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(ChisqTest_DepPA)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 4\n  statistic       p.value parameter method                    \n      <dbl>         <dbl>     <int> <chr>                     \n1      41.2 0.00000000115         2 Pearson's Chi-squared test\n```\n:::\n\n```{.r .cell-code}\ntidy(ChisqTest_DepPA)$p.value\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.147897e-09\n```\n:::\n:::\n\n\n\n\n\n## Observed & expected counts in R\n\n::: columns\n::: {.column width=\"50%\"}\n\nYou can see what the __observed__ and __expected__ counts are from the saved chi-squared test results:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nChisqTest_DepPA$observed\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         \n           No Yes\n  Most     27   1\n  None    115 199\n  Several  32  26\n```\n:::\n\n```{.r .cell-code}\nChisqTest_DepPA$expected\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         \n              No    Yes\n  Most     12.18  15.82\n  None    136.59 177.41\n  Several  25.23  32.77\n```\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n![](/img_slides/ChiSq_Table_Expected_brief_DepressionPA.png){fig-align=\"center\"}\n\n* Why is it important to look at the expected counts?\n\n* What are we looking for in the expected counts?\n\n:::\n:::\n\n\n\n## $\\chi^2$ test in R with 2-way table (1/2)\n\nCreate a base R table of the results:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(DepPA_table <- matrix(c(199, 26, 1, 115, 32, 27), nrow = 2, ncol = 3, byrow = T))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]  199   26    1\n[2,]  115   32   27\n```\n:::\n\n```{.r .cell-code}\ndimnames(DepPA_table) <- list(\"PA\" = c(\"Yes\", \"No\"),   # row names\n                              \"Depression\" = c(\"None\", \"Several\", \"Most\"))  # column names\nDepPA_table\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Depression\nPA    None Several Most\n  Yes  199      26    1\n  No   115      32   27\n```\n:::\n:::\n\n\n\n\n\n\n\n## $\\chi^2$ test in R with 2-way table (2/2)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchisq.test(DepPA_table) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPearson's Chi-squared test\n\ndata:  DepPA_table\nX-squared = 41.171, df = 2, p-value = 1.148e-09\n```\n:::\n\n```{.r .cell-code}\nchisq.test(DepPA_table)$expected\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Depression\nPA      None Several  Most\n  Yes 177.41   32.77 15.82\n  No  136.59   25.23 12.18\n```\n:::\n:::\n\n\n\n\n\n## Continuity correction\n\n* Just like the proportions test, $\\chi^2$ tests have the option of including a continuity correction\n* The __default includes a continuity correction__\n    * Output below is without a CC - it looks the same for this example \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchisq.test(DepPA_table, correct = FALSE) %>% tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 4\n  statistic       p.value parameter method                    \n      <dbl>         <dbl>     <int> <chr>                     \n1      41.2 0.00000000115         2 Pearson's Chi-squared test\n```\n:::\n:::\n\n\n\n\n\n\n# Fischer's Exact Test\n\nUse this if sample size too small\n\n\n\n## Example with small sample sizes \n\n* Suppose that instead of taking a random sample of 400 adults (from the NHANES data), a study takes a random sample of 100 such that\n    * 50 people that are physically active and \n    * 50 people that are not physically active \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(DepPA100_table <- matrix(c(43, 5, 2, 40, 4, 6), nrow = 2, ncol = 3, byrow = T))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]   43    5    2\n[2,]   40    4    6\n```\n:::\n\n```{.r .cell-code}\ndimnames(DepPA100_table) <- list(\"PA\" = c(\"Yes\", \"No\"),   # row names\n                              \"Depression\" = c(\"None\", \"Several\", \"Most\"))  # column names\nDepPA100_table\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Depression\nPA    None Several Most\n  Yes   43       5    2\n  No    40       4    6\n```\n:::\n:::\n\n\n\n\n## Chi-squared test warning (1/2)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchisq.test(DepPA100_table) \n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in stats::chisq.test(x, y, ...): Chi-squared approximation may be\nincorrect\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPearson's Chi-squared test\n\ndata:  DepPA100_table\nX-squared = 2.2195, df = 2, p-value = 0.3296\n```\n:::\n:::\n\n\n\n\n\n## Chi-squared test warning (2/2)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchisq.test(DepPA100_table)$expected\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in stats::chisq.test(x, y, ...): Chi-squared approximation may be\nincorrect\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n     Depression\nPA    None Several Most\n  Yes 41.5     4.5    4\n  No  41.5     4.5    4\n```\n:::\n:::\n\n\n\n* Recall the __sample size__ condition\n    * In order for the test statistic to be modeled by a chi-squared distribution we need\n    * __[ 2 $\\times$ 2 table:]{style=\"color:darkorange\"} expected counts are at least 10 for each cell__\n    * __[larger tables:   ]{style=\"color:darkorange\"}__\n        * __no more than 1/5 of the expected counts are less than 5__, and \n        * __all expected counts are greater than 1__\n\n\n\n## Fisher's Exact Test\n\n* Called an exact test since it \n    * calculates an exact p-value probability \n        * instead of using an asymptotic approximation, such as the normal, t, or chi-squared distributions\n    * For 2x2 tables the p-value is calculated using the hypergeometric probability distribution (see book for details)  \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfisher.test(DepPA100_table)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tFisher's Exact Test for Count Data\n\ndata:  DepPA100_table\np-value = 0.3844\nalternative hypothesis: two.sided\n```\n:::\n:::\n\n\n\nNote: no test statistic & a two-sided test\n\n\n## Another option: simulate p-value\n\nFrom the help file: \n\n* Simulation is done by random sampling from the set of all contingency tables with given marginals, and \n    * works only if the marginals are strictly positive. \n* Continuity correction is never used, and the statistic is quoted without it. \n* Note that this is not the usual sampling situation assumed for the chi-squared test but rather that for Fisher's exact test.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchisq.test(DepPA100_table, simulate.p.value = TRUE) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPearson's Chi-squared test with simulated p-value (based on 2000\n\treplicates)\n\ndata:  DepPA100_table\nX-squared = 2.2195, df = NA, p-value = 0.3913\n```\n:::\n:::\n\n\n\n\n\n\n\n# $\\chi^2$ test vs. testing proportions\n\n\n\n\n## $\\chi^2$ test vs. testing differences in proportions (1/2)\n\nIf there are only 2 levels in both of the categorical variables being tested, then the _p_-value from the $\\chi^2$ test is equal to the _p_-value from the differences in proportions test.\n\n__Example:__ On Day 15 we tested whether the proportion who had participated in sports betting was the same for college and noncollege young adults:\n\n\\begin{align}\nH_0:& ~p_{coll} - p_{noncoll} = 0\\\\\nH_A:& ~p_{coll} - p_{noncoll} \\neq 0\n\\end{align}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSportsBet_table <- matrix(c(175, 94, 137, 77), nrow = 2, ncol = 2, byrow = T)\ndimnames(SportsBet_table) <- list(\"Group\" = c(\"College\", \"NonCollege\"),   # row names\n                              \"Bet\" = c(\"No\", \"Yes\"))  # column names\nSportsBet_table\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Bet\nGroup         No Yes\n  College    175  94\n  NonCollege 137  77\n```\n:::\n:::\n\n\n\n\n\n## $\\chi^2$ test vs. testing differences in proportions (2/2)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchisq.test(SportsBet_table) %>% tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 4\n  statistic p.value parameter method                                            \n      <dbl>   <dbl>     <int> <chr>                                             \n1    0.0199   0.888         1 Pearson's Chi-squared test with Yates' continuity…\n```\n:::\n\n```{.r .cell-code}\nprop.test(SportsBet_table) %>% tidy() \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 9\n  estimate1 estimate2 statistic p.value parameter conf.low conf.high method     \n      <dbl>     <dbl>     <dbl>   <dbl>     <dbl>    <dbl>     <dbl> <chr>      \n1     0.651     0.640    0.0199   0.888         1  -0.0797     0.100 2-sample t…\n# ℹ 1 more variable: alternative <chr>\n```\n:::\n\n```{.r .cell-code}\nz <- sqrt(0.0199)\n2*pnorm(z, lower.tail=F)# p-value\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8878167\n```\n:::\n:::\n\n\n\n<!-- * Same test statistic (both shown as $\\chi^2$) and _p_-value! -->\n<!-- * This is because $Z^2 \\sim \\chi_{df=1}^2$, if $Z\\sim N(0,1)$. -->\n\n\n\n## Test of Homogeneity\n\n* Running the sports betting example as a chi-squared test is actually an example of a __test of homogeneity__\n\n* In a test of homogeneity, proportions can be compared between many groups\n\n\\begin{align}\nH_0:&~ p_1 = p_2 = p_2 = \\ldots = p_n\\\\\nH_A:&~ p_i \\neq p_j \\textrm{for at least one pair of } i, j\n\\end{align}\n\n* It's an extension of a two proportions test.\n\n* The test statistic & p-value are calculated the same was as a chi-squared test of association (independence)\n\n* When we fix the margins (whether row or columns) of one of the \"variables\" (such as in a cohort or case-control study)\n    * the chi-squared test is called a __Test of Homogeneity__\n\n\n## Overview of tests with categorical outcome\n\n<br>\n<br>\n\n![](/img_slides/flowchart_only_categorical.png){fig-align=\"center\"}\n\n\n## Chi-squared Tests of Independence vs. Homogeneity vs. Goodness-of-fit\n\n![](/img_slides/chisq_tests_different_types_TileStats.png){fig-align=\"center\"}\n\n* See YouTube video from TileStats for a good explanation of how these three tests are different: <https://www.youtube.com/watch?v=TyD-_1JUhxw>\n* UCLA's INSPIRE website has a good summary too: <http://inspire.stat.ucla.edu/unit_13/>\n\n\n## What's next?\n\n<br>\n<br>\n\n![](/img_slides/flowchart_511_continuous_categorical.png){fig-align=\"center\"}",
    "supporting": [
      "Day13_bsta511_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}